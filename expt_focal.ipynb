{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eff26d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3ecbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794e832c",
   "metadata": {},
   "source": [
    "# Code 1: Retrain ViT with Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb2f5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(48)\n",
    "torch.cuda.manual_seed(48)\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=None):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = torch.nn.functional.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        loss = (1 - pt) ** self.gamma * ce_loss\n",
    "        if self.alpha is not None:\n",
    "            loss = self.alpha[targets] * loss\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "exp_name = \"expt_1\"\n",
    "os.makedirs(exp_name, exist_ok=True)\n",
    "\n",
    "\n",
    "data_dir = \"NSD\"\n",
    "train_dataset = datasets.ImageFolder(os.path.join(data_dir, \"train\"), transform=transform)\n",
    "val_dataset = datasets.ImageFolder(os.path.join(data_dir, \"val\"), transform=transform)\n",
    "test_dataset = datasets.ImageFolder(os.path.join(data_dir, \"test\"), transform=transform)\n",
    "\n",
    "# Simulate imbalance: ashgourd_fresh (class 0) to 5 samples\n",
    "min_class = 0\n",
    "min_indices = [i for i, (_, lbl) in enumerate(train_dataset.samples) if lbl == min_class]\n",
    "keep_indices = torch.randperm(len(min_indices))[:5].tolist()  # Randomly select 5\n",
    "keep_indices = [min_indices[i] for i in keep_indices]\n",
    "all_indices = [i for i in range(len(train_dataset)) if i not in min_indices] + keep_indices\n",
    "imbalanced_train_dataset = Subset(train_dataset, all_indices)\n",
    "\n",
    "train_loader = DataLoader(imbalanced_train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "bundle_path = \"expt_1/vit_tiny_model_bundle_focal.pth\"\n",
    "bundle = torch.load(bundle_path, map_location=\"cpu\", weights_only=False)\n",
    "vit_model = timm.create_model(bundle[\"model_name\"], pretrained=False, num_classes=bundle[\"num_classes\"])\n",
    "vit_model.load_state_dict(bundle[\"state_dict\"])\n",
    "\n",
    "\n",
    "class ViTMAEModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = vit_model\n",
    "        self.criterion = FocalLoss(gamma=2.0, alpha=None)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        self.log(\"train_loss\", loss, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log(\"val_loss\", loss, on_epoch=True)\n",
    "        self.log(\"val_acc\", acc, on_epoch=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log(\"test_loss\", loss, on_epoch=True)\n",
    "        self.log(\"test_acc\", acc, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=exp_name,\n",
    "    filename=\"best_vit_tiny_patch16_224_focal\",\n",
    "    monitor=\"val_acc\",\n",
    "    mode=\"max\",\n",
    "    save_top_k=1\n",
    ")\n",
    "logger = CSVLogger(save_dir=exp_name, name=\"logs\")\n",
    "\n",
    "\n",
    "model = ViTMAEModel()\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    accelerator=\"auto\",\n",
    "    callbacks=[checkpoint_callback],\n",
    "    logger=logger,\n",
    "    default_root_dir=exp_name,\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.test(model, test_loader)\n",
    "\n",
    "\n",
    "best_checkpoint = f\"{exp_name}/best_vit_tiny_patch16_224_focal.ckpt\"\n",
    "if os.path.exists(best_checkpoint):\n",
    "    vit_model = ViTMAEModel.load_from_checkpoint(best_checkpoint).model\n",
    "    vit_bundle = {\n",
    "        \"model_name\": \"vit_tiny_patch16_224\",\n",
    "        \"state_dict\": vit_model.state_dict(),\n",
    "        \"transform\": transform,\n",
    "        \"num_classes\": 9\n",
    "    }\n",
    "    torch.save(vit_bundle, f\"{exp_name}/vit_tiny_model_bundle_focal.pth\")\n",
    "    print(f\"Saved bundle to {exp_name}/vit_tiny_model_bundle_focal.pth\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Checkpoint not found at {best_checkpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3de179",
   "metadata": {},
   "source": [
    "# Code 2: Retrain ResNet with Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca219110",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(48)\n",
    "torch.cuda.manual_seed(48)\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=None):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = torch.nn.functional.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        loss = (1 - pt) ** self.gamma * ce_loss\n",
    "        if self.alpha is not None:\n",
    "            loss = self.alpha[targets] * loss\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "exp_name = \"expt_1\"\n",
    "os.makedirs(exp_name, exist_ok=True)\n",
    "\n",
    "\n",
    "data_dir = \"NSD\"\n",
    "train_dataset = datasets.ImageFolder(os.path.join(data_dir, \"train\"), transform=transform)\n",
    "val_dataset = datasets.ImageFolder(os.path.join(data_dir, \"val\"), transform=transform)\n",
    "test_dataset = datasets.ImageFolder(os.path.join(data_dir, \"test\"), transform=transform)\n",
    "\n",
    "# Simulate imbalance: ashgourd_fresh (class 0) to 5 samples\n",
    "min_class = 0\n",
    "min_indices = [i for i, (_, lbl) in enumerate(train_dataset.samples) if lbl == min_class]\n",
    "keep_indices = torch.randperm(len(min_indices))[:5].tolist()  # Randomly select 5\n",
    "keep_indices = [min_indices[i] for i in keep_indices]\n",
    "all_indices = [i for i in range(len(train_dataset)) if i not in min_indices] + keep_indices\n",
    "imbalanced_train_dataset = Subset(train_dataset, all_indices)\n",
    "\n",
    "train_loader = DataLoader(imbalanced_train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "bundle_path = \"expt_1/resnet50_model_bundle_focal.pth\"\n",
    "bundle = torch.load(bundle_path, map_location=\"cpu\", weights_only=False)\n",
    "resnet_model = models.resnet50(weights=None)\n",
    "resnet_model.fc = nn.Linear(resnet_model.fc.in_features, bundle[\"num_classes\"])\n",
    "resnet_model.load_state_dict(bundle[\"state_dict\"])\n",
    "\n",
    "\n",
    "class ResNetModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = resnet_model\n",
    "        self.criterion = FocalLoss(gamma=2.0, alpha=None)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        self.log(\"test_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-5)  \n",
    "\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=exp_name,\n",
    "    filename=\"best_resnet50_focal\",\n",
    "    monitor=\"val_acc\",\n",
    "    mode=\"max\",\n",
    "    save_top_k=1\n",
    ")\n",
    "logger = CSVLogger(save_dir=exp_name, name=\"logs\")\n",
    "\n",
    "\n",
    "model = ResNetModel()\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    accelerator=\"auto\",\n",
    "    callbacks=[checkpoint_callback],\n",
    "    logger=logger,\n",
    "    default_root_dir=exp_name,\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.test(model, test_loader)\n",
    "\n",
    "\n",
    "best_checkpoint = f\"{exp_name}/best_resnet50_focal.ckpt\"\n",
    "if os.path.exists(best_checkpoint):\n",
    "    resnet_model = ResNetModel.load_from_checkpoint(best_checkpoint).model\n",
    "    resnet_bundle = {\n",
    "        \"model_name\": \"resnet50\",\n",
    "        \"state_dict\": resnet_model.state_dict(),\n",
    "        \"transform\": transform,\n",
    "        \"num_classes\": 9\n",
    "    }\n",
    "    torch.save(resnet_bundle, f\"{exp_name}/resnet50_model_bundle_focal.pth\")\n",
    "    print(f\"Saved bundle to {exp_name}/resnet50_model_bundle_focal.pth\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Checkpoint not found at {best_checkpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e54756",
   "metadata": {},
   "source": [
    "# Code 3: MobileVit with focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb24d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(48)\n",
    "torch.cuda.manual_seed(48)\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=None):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = torch.nn.functional.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        loss = (1 - pt) ** self.gamma * ce_loss\n",
    "        if self.alpha is not None:\n",
    "            loss = self.alpha[targets] * loss\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "exp_name = \"expt_1\"\n",
    "os.makedirs(exp_name, exist_ok=True)\n",
    "\n",
    "# Load imbalanced dataset (ashgourd_fresh: 5 samples)\n",
    "data_dir = \"NSD\"\n",
    "train_dataset = datasets.ImageFolder(os.path.join(data_dir, \"train\"), transform=transform)\n",
    "val_dataset = datasets.ImageFolder(os.path.join(data_dir, \"val\"), transform=transform)\n",
    "test_dataset = datasets.ImageFolder(os.path.join(data_dir, \"test\"), transform=transform)\n",
    "\n",
    "# Simulate imbalance: ashgourd_fresh (class 0) to 5 samples\n",
    "min_class = 0  \n",
    "min_indices = [i for i, (_, lbl) in enumerate(train_dataset.samples) if lbl == min_class]\n",
    "keep_indices = torch.randperm(len(min_indices))[:5].tolist()  # Randomly select 5\n",
    "keep_indices = [min_indices[i] for i in keep_indices]\n",
    "all_indices = [i for i in range(len(train_dataset)) if i not in min_indices] + keep_indices\n",
    "imbalanced_train_dataset = Subset(train_dataset, all_indices)\n",
    "\n",
    "train_loader = DataLoader(imbalanced_train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "bundle_path = \"expt_1/mobilevit_xs_model_bundle.pth\"\n",
    "bundle = torch.load(bundle_path, map_location=\"cpu\", weights_only=False)\n",
    "vit_model = timm.create_model(\"mobilevit_xs\", pretrained=False, num_classes=bundle[\"num_classes\"])\n",
    "vit_model.load_state_dict(bundle[\"state_dict\"])\n",
    "\n",
    "\n",
    "class ViTMAEModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = vit_model\n",
    "        self.criterion = FocalLoss(gamma=2.0, alpha=None)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        self.log(\"train_loss\", loss, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log(\"val_loss\", loss, on_epoch=True)\n",
    "        self.log(\"val_acc\", acc, on_epoch=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log(\"test_loss\", loss, on_epoch=True)\n",
    "        self.log(\"test_acc\", acc, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-5)  \n",
    "\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=exp_name,\n",
    "    filename=\"best_mobilevit_xs_focal\",\n",
    "    monitor=\"val_acc\",\n",
    "    mode=\"max\",\n",
    "    save_top_k=1\n",
    ")\n",
    "logger = CSVLogger(save_dir=exp_name, name=\"logs\")\n",
    "\n",
    "\n",
    "model = ViTMAEModel()\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    accelerator=\"auto\",\n",
    "    callbacks=[checkpoint_callback],\n",
    "    logger=logger,\n",
    "    default_root_dir=exp_name,\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.test(model, test_loader)\n",
    "\n",
    "\n",
    "best_checkpoint = f\"{exp_name}/best_mobilevit_xs_focal.ckpt\"\n",
    "if os.path.exists(best_checkpoint):\n",
    "    vit_model = ViTMAEModel.load_from_checkpoint(best_checkpoint).model\n",
    "    vit_bundle = {\n",
    "        \"model_name\": \"mobilevit_xs\",\n",
    "        \"state_dict\": vit_model.state_dict(),\n",
    "        \"transform\": transform,\n",
    "        \"num_classes\": 9\n",
    "    }\n",
    "    torch.save(vit_bundle, f\"{exp_name}/mobilevit_xs_model_bundle_focal.pth\")\n",
    "    print(f\"Saved bundle to {exp_name}/mobilevit_xs_model_bundle_focal.pth\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Checkpoint not found at {best_checkpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37e95c2",
   "metadata": {},
   "source": [
    "# Code 3.5: Testing balanced and imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6eb8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import timm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "torch.manual_seed(48)\n",
    "np.random.seed(48)\n",
    "\n",
    "def load_model_bundle(bundle_path, model_name):\n",
    "    bundle = torch.load(bundle_path, map_location=\"cpu\", weights_only=False)\n",
    "    if bundle[\"model_name\"] != model_name:\n",
    "        raise ValueError(f\"Expected {model_name}, got {bundle['model_name']}\")\n",
    "    \n",
    "    if model_name == \"resnet50\":\n",
    "        model = models.resnet50(pretrained=False)\n",
    "        model.fc = nn.Linear(model.fc.in_features, bundle[\"num_classes\"])\n",
    "    else:\n",
    "        model = timm.create_model(model_name, pretrained=False, num_classes=bundle[\"num_classes\"])\n",
    "    \n",
    "    model.load_state_dict(bundle[\"state_dict\"])\n",
    "    model.eval()\n",
    "    return model, bundle[\"transform\"]\n",
    "\n",
    "def evaluate(model, loader, device, desc=\"\"):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for images, lbls in loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "            labels.extend(lbls.numpy())\n",
    "    inference_time = time.time() - start_time\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(labels, preds) * 100,\n",
    "        \"precision\": precision_score(labels, preds, average=\"macro\"),\n",
    "        \"recall\": recall_score(labels, preds, average=\"macro\"),\n",
    "        \"f1\": f1_score(labels, preds, average=\"macro\"),\n",
    "        \"inference_time\": inference_time\n",
    "    }\n",
    "    return preds, labels, metrics\n",
    "\n",
    "\n",
    "exp_name = \"expt_1\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_dir = \"NSD\"\n",
    "test_dataset = datasets.ImageFolder(os.path.join(data_dir, \"test\"), transform=None)\n",
    "\n",
    "\n",
    "min_class = 0\n",
    "min_indices = [i for i, (_, lbl) in enumerate(test_dataset.samples) if lbl == min_class]\n",
    "keep_indices = np.random.choice(min_indices, size=5, replace=False)\n",
    "all_indices = [i for i in range(len(test_dataset)) if i not in min_indices] + list(keep_indices)\n",
    "imbalanced_dataset = Subset(test_dataset, all_indices)\n",
    "\n",
    "\n",
    "models_config = [\n",
    "    {\"name\": \"ViT-Tiny\", \"model_name\": \"vit_tiny_patch16_224\", \"bundle_path\": f\"{exp_name}/vit_tiny_model_bundle_focal.pth\"},\n",
    "    {\"name\": \"ResNet-50\", \"model_name\": \"resnet50\", \"bundle_path\": f\"{exp_name}/resnet50_model_bundle_focal.pth\"},\n",
    "    {\"name\": \"MobileViT-XS\", \"model_name\": \"mobilevit_xs\", \"bundle_path\": f\"{exp_name}/mobilevit_xs_model_bundle_focal.pth\"}\n",
    "]\n",
    "\n",
    "\n",
    "all_metrics = {}\n",
    "mean, std = np.array([0.485, 0.456, 0.406]), np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "for config in models_config:\n",
    "    name, model_name, bundle_path = config[\"name\"], config[\"model_name\"], config[\"bundle_path\"]\n",
    "    print(f\"\\nTesting {name}...\")\n",
    "    \n",
    "    \n",
    "    model, transform = load_model_bundle(bundle_path, model_name)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    \n",
    "    test_dataset.transform = transform\n",
    "    imbalanced_dataset.dataset.transform = transform\n",
    "    \n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "    imbalanced_loader = DataLoader(imbalanced_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "    \n",
    "    \n",
    "    _, _, balanced_metrics = evaluate(model, test_loader, device, \"Balanced Test\")\n",
    "    _, _, imbalanced_metrics = evaluate(model, imbalanced_loader, device, \"Imbalanced Test\")\n",
    "    \n",
    "    \n",
    "    metrics = {\n",
    "        \"balanced\": balanced_metrics,\n",
    "        \"imbalanced\": imbalanced_metrics\n",
    "    }\n",
    "    all_metrics[name] = metrics\n",
    "    \n",
    "    \n",
    "    print(f\"{name} Results:\")\n",
    "    for dataset_type, metric_values in metrics.items():\n",
    "        print(f\"\\n{dataset_type.capitalize()} Dataset:\")\n",
    "        for k, v in metric_values.items():\n",
    "            unit = \"s\" if \"time\" in k else \"\"\n",
    "            print(f\"  {k.capitalize()}: {v:.2f}{unit}\")\n",
    "    \n",
    "    \n",
    "    os.makedirs(exp_name, exist_ok=True)\n",
    "    json_path = f\"{exp_name}/{model_name}_focal_test_bal_imb_results.json\"\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    print(f\"Saved metrics to {json_path}\")\n",
    "    \n",
    "    \n",
    "    display_images, display_preds, display_labels = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "            images = images.to(device)\n",
    "            preds = model(images).argmax(dim=1).cpu().numpy()\n",
    "            if i == 0:\n",
    "                display_images = images[:5].cpu()\n",
    "                display_preds = preds[:5]\n",
    "                display_labels = labels[:5].numpy()\n",
    "            break\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 5, figsize=(15, 3))\n",
    "    for i, (img, pred, true) in enumerate(zip(display_images, display_preds, display_labels)):\n",
    "        img = img.permute(1, 2, 0).numpy() * std + mean\n",
    "        img = np.clip(img, 0, 1)\n",
    "        axs[i].imshow(img)\n",
    "        axs[i].set_title(f\"Pred: {pred}\\nTrue: {true}\")\n",
    "        axs[i].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    img_path = f\"{exp_name}/{model_name}_prediction_examples.png\"\n",
    "    plt.savefig(img_path)\n",
    "    plt.show()\n",
    "    print(f\"Saved predictions to {img_path}\")\n",
    "\n",
    "\n",
    "combined_json_path = f\"{exp_name}/combined_focal_test_bal_imb_results.json\"\n",
    "with open(combined_json_path, \"w\") as f:\n",
    "    json.dump(all_metrics, f, indent=4)\n",
    "print(f\"\\nSaved combined metrics to {combined_json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae3280c",
   "metadata": {},
   "source": [
    "# Code 4: Feature Visualization Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6caed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_bundle_path = \"expt_1/vit_tiny_model_bundle_focal.pth\"\n",
    "resnet_bundle_path = \"expt_1/resnet50_model_bundle_focal.pth\"\n",
    "mobilevit_bundle_path = \"expt_1/mobilevit_xs_model_bundle_focal.pth\"\n",
    "data_dir = \"NSD/test\"\n",
    "output_dir = \"expt_1/focal_imbalance_feature_visualization\"\n",
    "\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def set_seed(seed=48):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()\n",
    "\n",
    "\n",
    "def get_feature_extractor(model, model_type):\n",
    "    \"\"\"\n",
    "    Returns a feature extractor function for the given model type\n",
    "    \"\"\"\n",
    "    if model_type == \"vit\":\n",
    "        \n",
    "        def extract_features(x):\n",
    "            \n",
    "            x = model.patch_embed(x)\n",
    "            cls_token = model.cls_token.expand(x.shape[0], -1, -1)\n",
    "            x = torch.cat((cls_token, x), dim=1)\n",
    "            if hasattr(model, 'pos_drop'):\n",
    "                x = model.pos_drop(x + model.pos_embed)\n",
    "            else:\n",
    "                x = x + model.pos_embed\n",
    "                \n",
    "            \n",
    "            for blk in model.blocks:\n",
    "                x = blk(x)\n",
    "                \n",
    "            x = model.norm(x)\n",
    "            \n",
    "            return x[:, 0]\n",
    "            \n",
    "    elif model_type == \"mobilevit\":\n",
    "        \n",
    "        def extract_features(x):\n",
    "            \n",
    "            x = model.stem(x)\n",
    "            for stage in model.stages:\n",
    "                x = stage(x)\n",
    "            \n",
    "            x = nn.functional.adaptive_avg_pool2d(x, (1, 1))\n",
    "            x = x.flatten(1)  \n",
    "            return x\n",
    "            \n",
    "    elif model_type == \"resnet\":\n",
    "        \n",
    "        def extract_features(x):\n",
    "            x = model.conv1(x)\n",
    "            x = model.bn1(x)\n",
    "            x = model.relu(x)\n",
    "            x = model.maxpool(x)\n",
    "            \n",
    "            x = model.layer1(x)\n",
    "            x = model.layer2(x)\n",
    "            x = model.layer3(x)\n",
    "            x = model.layer4(x)\n",
    "            \n",
    "            x = model.avgpool(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            return x\n",
    "    \n",
    "    return extract_features\n",
    "\n",
    "def load_model_bundle(bundle_path):\n",
    "    \"\"\"\n",
    "    Load a model bundle and return the model, transform, and model type\n",
    "    \"\"\"\n",
    "    bundle = torch.load(bundle_path, map_location=\"cpu\", weights_only=False)\n",
    "    model_name = bundle[\"model_name\"]\n",
    "    \n",
    "    if \"vit\" in model_name or \"mobilevit\" in model_name:\n",
    "        model = timm.create_model(model_name, pretrained=False, num_classes=bundle[\"num_classes\"])\n",
    "        model_type = \"vit\" if \"vit\" in model_name and \"mobilevit\" not in model_name else \"mobilevit\"\n",
    "    elif \"resnet\" in model_name:\n",
    "        model = models.resnet50(pretrained=False)\n",
    "        model.fc = nn.Linear(model.fc.in_features, bundle[\"num_classes\"])\n",
    "        model_type = \"resnet\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_name}\")\n",
    "    \n",
    "    model.load_state_dict(bundle[\"state_dict\"])\n",
    "    model.eval()\n",
    "    \n",
    "    return model, bundle[\"transform\"], model_type\n",
    "\n",
    "def extract_features(model, feature_extractor, data_loader, device):\n",
    "    \"\"\"\n",
    "    Extract features from a model using the provided feature extractor\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images = images.to(device)\n",
    "            batch_features = feature_extractor(images)\n",
    "            features.append(batch_features.cpu().numpy())\n",
    "            labels.append(targets.numpy())\n",
    "    \n",
    "    features = np.concatenate(features, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "def calculate_separation_metrics(features, labels):\n",
    "    \"\"\"\n",
    "    Calculate separation metrics for feature space:\n",
    "    - Average intra-class distance\n",
    "    - Average inter-class distance\n",
    "    - Separation ratio (inter/intra)\n",
    "    \"\"\"\n",
    "    classes = np.unique(labels)\n",
    "    \n",
    "    \n",
    "    centers = []\n",
    "    for c in classes:\n",
    "        centers.append(np.mean(features[labels == c], axis=0))\n",
    "    centers = np.array(centers)\n",
    "    \n",
    "    \n",
    "    intra_class_distances = []\n",
    "    for c in classes:\n",
    "        class_features = features[labels == c]\n",
    "        center = centers[int(c)]\n",
    "        distances = np.sqrt(np.sum((class_features - center)**2, axis=1))\n",
    "        intra_class_distances.append(np.mean(distances))\n",
    "    \n",
    "    avg_intra_class_distance = np.mean(intra_class_distances)\n",
    "    \n",
    "    \n",
    "    inter_class_distances = []\n",
    "    for i in range(len(centers)):\n",
    "        for j in range(i+1, len(centers)):\n",
    "            distance = np.sqrt(np.sum((centers[i] - centers[j])**2))\n",
    "            inter_class_distances.append(distance)\n",
    "    \n",
    "    avg_inter_class_distance = np.mean(inter_class_distances)\n",
    "    \n",
    "    \n",
    "    separation_ratio = avg_inter_class_distance / avg_intra_class_distance\n",
    "    \n",
    "    return {\n",
    "        'avg_intra_class_distance': avg_intra_class_distance,\n",
    "        'avg_inter_class_distance': avg_inter_class_distance,\n",
    "        'separation_ratio': separation_ratio\n",
    "    }\n",
    "\n",
    "def plot_class_distribution(labels, class_names, title, filename=None):\n",
    "    \"\"\"\n",
    "    Plot class distribution\n",
    "    \"\"\"\n",
    "    class_counts = np.bincount(labels, minlength=len(class_names))\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(len(class_names)), class_counts)\n",
    "    plt.xticks(range(len(class_names)), class_names, rotation=45)\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(title)\n",
    "    \n",
    "    if filename:\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(\"Loading ViT model...\")\n",
    "vit_model, vit_transform, vit_model_type = load_model_bundle(vit_bundle_path)\n",
    "vit_model = vit_model.to(device)\n",
    "vit_feature_extractor = get_feature_extractor(vit_model, vit_model_type)\n",
    "\n",
    "print(\"Loading ResNet model...\")\n",
    "resnet_model, resnet_transform, resnet_model_type = load_model_bundle(resnet_bundle_path)\n",
    "resnet_model = resnet_model.to(device)\n",
    "resnet_feature_extractor = get_feature_extractor(resnet_model, resnet_model_type)\n",
    "\n",
    "print(\"Loading MobileViT model...\")\n",
    "mobilevit_model, mobilevit_transform, mobilevit_model_type = load_model_bundle(mobilevit_bundle_path)\n",
    "mobilevit_model = mobilevit_model.to(device)\n",
    "mobilevit_feature_extractor = get_feature_extractor(mobilevit_model, mobilevit_model_type)\n",
    "\n",
    "print(\"Loading full dataset...\")\n",
    "full_dataset = datasets.ImageFolder(data_dir, transform=vit_transform)\n",
    "class_names = full_dataset.classes\n",
    "class_counts = np.bincount([label for _, label in full_dataset.samples])\n",
    "print(\"Class distribution:\", dict(zip(class_names, class_counts)))\n",
    "\n",
    "print(\"Creating imbalanced dataset...\")\n",
    "min_class = np.argmin(class_counts)\n",
    "print(f\"Minority class: {class_names[min_class]} with {class_counts[min_class]} samples\")\n",
    "min_indices = [i for i, (_, lbl) in enumerate(full_dataset.samples) if lbl == min_class]\n",
    "min_keep_indices = np.random.choice(min_indices, size=len(min_indices) // 2, replace=False)\n",
    "other_indices = [i for i, (_, lbl) in enumerate(full_dataset.samples) if lbl != min_class]\n",
    "imbalanced_indices = other_indices + list(min_keep_indices)\n",
    "imbalanced_dataset = Subset(full_dataset, imbalanced_indices)\n",
    "\n",
    "batch_size = 32\n",
    "full_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "imbalanced_loader = DataLoader(imbalanced_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "print(\"Extracting features from full dataset...\")\n",
    "vit_features_full, vit_labels_full = extract_features(vit_model, vit_feature_extractor, full_loader, device)\n",
    "resnet_features_full, resnet_labels_full = extract_features(resnet_model, resnet_feature_extractor, full_loader, device)\n",
    "mobilevit_features_full, mobilevit_labels_full = extract_features(mobilevit_model, mobilevit_feature_extractor, full_loader, device)\n",
    "\n",
    "print(\"Extracting features from imbalanced dataset...\")\n",
    "vit_features_imbalanced, vit_labels_imbalanced = extract_features(vit_model, vit_feature_extractor, imbalanced_loader, device)\n",
    "resnet_features_imbalanced, resnet_labels_imbalanced = extract_features(resnet_model, resnet_feature_extractor, imbalanced_loader, device)\n",
    "mobilevit_features_imbalanced, mobilevit_labels_imbalanced = extract_features(mobilevit_model, mobilevit_feature_extractor, imbalanced_loader, device)\n",
    "\n",
    "plot_class_distribution(vit_labels_full, class_names, \"Full Dataset Class Distribution\", \n",
    "                        os.path.join(output_dir, \"full_class_distribution.png\"))\n",
    "plot_class_distribution(vit_labels_imbalanced, class_names, \"Imbalanced Dataset Class Distribution\", \n",
    "                        os.path.join(output_dir, \"imbalanced_class_distribution.png\"))\n",
    "\n",
    "print(\"Applying PCA to full dataset features...\")\n",
    "vit_pca_full = PCA(n_components=2)\n",
    "vit_pca_full_result = vit_pca_full.fit_transform(vit_features_full)\n",
    "resnet_pca_full = PCA(n_components=2)\n",
    "resnet_pca_full_result = resnet_pca_full.fit_transform(resnet_features_full)\n",
    "mobilevit_pca_full = PCA(n_components=2)\n",
    "mobilevit_pca_full_result = mobilevit_pca_full.fit_transform(mobilevit_features_full)\n",
    "\n",
    "print(\"Applying PCA to imbalanced dataset features...\")\n",
    "vit_pca_imbalanced = PCA(n_components=2)\n",
    "vit_pca_imbalanced_result = vit_pca_imbalanced.fit_transform(vit_features_imbalanced)\n",
    "resnet_pca_imbalanced = PCA(n_components=2)\n",
    "resnet_pca_imbalanced_result = resnet_pca_imbalanced.fit_transform(resnet_features_imbalanced)\n",
    "mobilevit_pca_imbalanced = PCA(n_components=2)\n",
    "mobilevit_pca_imbalanced_result = mobilevit_pca_imbalanced.fit_transform(mobilevit_features_imbalanced)\n",
    "\n",
    "print(\"Applying t-SNE to full dataset features (this may take a while)...\")\n",
    "vit_tsne_full = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "vit_tsne_full_result = vit_tsne_full.fit_transform(vit_features_full)\n",
    "resnet_tsne_full = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "resnet_tsne_full_result = resnet_tsne_full.fit_transform(resnet_features_full)\n",
    "mobilevit_tsne_full = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "mobilevit_tsne_full_result = mobilevit_tsne_full.fit_transform(mobilevit_features_full)\n",
    "\n",
    "print(\"Applying t-SNE to imbalanced dataset features (this may take a while)...\")\n",
    "vit_tsne_imbalanced = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "vit_tsne_imbalanced_result = vit_tsne_imbalanced.fit_transform(vit_features_imbalanced)\n",
    "resnet_tsne_imbalanced = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "resnet_tsne_imbalanced_result = resnet_tsne_imbalanced.fit_transform(resnet_features_imbalanced)\n",
    "mobilevit_tsne_imbalanced = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "mobilevit_tsne_imbalanced_result = mobilevit_tsne_imbalanced.fit_transform(mobilevit_features_imbalanced)\n",
    "\n",
    "plt.figure(figsize=(30, 10))\n",
    "\n",
    "# ViT PCA - Full Dataset\n",
    "plt.subplot(2, 3, 1)\n",
    "scatter = plt.scatter(vit_pca_full_result[:, 0], vit_pca_full_result[:, 1], c=vit_labels_full, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(scatter, boundaries=np.arange(len(class_names)+1)-0.5).set_ticks(np.arange(len(class_names)))\n",
    "plt.title('PCA: ViT features - Full Dataset', fontsize=14)\n",
    "plt.xlabel('Principal Component 1', fontsize=12)\n",
    "plt.ylabel('Principal Component 2', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# ViT PCA - Imbalanced Dataset\n",
    "plt.subplot(2, 3, 4)\n",
    "scatter = plt.scatter(vit_pca_imbalanced_result[:, 0], vit_pca_imbalanced_result[:, 1], c=vit_labels_imbalanced, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(scatter, boundaries=np.arange(len(class_names)+1)-0.5).set_ticks(np.arange(len(class_names)))\n",
    "plt.title('PCA: ViT features - Imbalanced Dataset', fontsize=14)\n",
    "plt.xlabel('Principal Component 1', fontsize=12)\n",
    "plt.ylabel('Principal Component 2', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# ResNet PCA - Full Dataset\n",
    "plt.subplot(2, 3, 2)\n",
    "scatter = plt.scatter(resnet_pca_full_result[:, 0], resnet_pca_full_result[:, 1], c=resnet_labels_full, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(scatter, boundaries=np.arange(len(class_names)+1)-0.5).set_ticks(np.arange(len(class_names)))\n",
    "plt.title('PCA: ResNet features - Full Dataset', fontsize=14)\n",
    "plt.xlabel('Principal Component 1', fontsize=12)\n",
    "plt.ylabel('Principal Component 2', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# ResNet PCA - Imbalanced Dataset\n",
    "plt.subplot(2, 3, 5)\n",
    "scatter = plt.scatter(resnet_pca_imbalanced_result[:, 0], resnet_pca_imbalanced_result[:, 1], c=resnet_labels_imbalanced, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(scatter, boundaries=np.arange(len(class_names)+1)-0.5).set_ticks(np.arange(len(class_names)))\n",
    "plt.title('PCA: ResNet features - Imbalanced Dataset', fontsize=14)\n",
    "plt.xlabel('Principal Component 1', fontsize=12)\n",
    "plt.ylabel('Principal Component 2', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# MobileViT PCA - Full Dataset\n",
    "plt.subplot(2, 3, 3)\n",
    "scatter = plt.scatter(mobilevit_pca_full_result[:, 0], mobilevit_pca_full_result[:, 1], c=mobilevit_labels_full, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(scatter, boundaries=np.arange(len(class_names)+1)-0.5).set_ticks(np.arange(len(class_names)))\n",
    "plt.title('PCA: MobileViT features - Full Dataset', fontsize=14)\n",
    "plt.xlabel('Principal Component 1', fontsize=12)\n",
    "plt.ylabel('Principal Component 2', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# MobileViT PCA - Imbalanced Dataset\n",
    "plt.subplot(2, 3, 6)\n",
    "scatter = plt.scatter(mobilevit_pca_imbalanced_result[:, 0], mobilevit_pca_imbalanced_result[:, 1], c=mobilevit_labels_imbalanced, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(scatter, boundaries=np.arange(len(class_names)+1)-0.5).set_ticks(np.arange(len(class_names)))\n",
    "plt.title('PCA: MobileViT features - Imbalanced Dataset', fontsize=14)\n",
    "plt.xlabel('Principal Component 1', fontsize=12)\n",
    "plt.ylabel('Principal Component 2', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'pca_comparison_imbalanced.png'))\n",
    "plt.close()\n",
    "\n",
    "# Plot t-SNE results for full vs imbalanced\n",
    "plt.figure(figsize=(30, 10))\n",
    "\n",
    "# ViT t-SNE - Full Dataset\n",
    "plt.subplot(2, 3, 1)\n",
    "scatter = plt.scatter(vit_tsne_full_result[:, 0], vit_tsne_full_result[:, 1], c=vit_labels_full, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(scatter, boundaries=np.arange(len(class_names)+1)-0.5).set_ticks(np.arange(len(class_names)))\n",
    "plt.title('t-SNE: ViT features - Full Dataset', fontsize=14)\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# ViT t-SNE - Imbalanced Dataset\n",
    "plt.subplot(2, 3, 4)\n",
    "scatter = plt.scatter(vit_tsne_imbalanced_result[:, 0], vit_tsne_imbalanced_result[:, 1], c=vit_labels_imbalanced, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(scatter, boundaries=np.arange(len(class_names)+1)-0.5).set_ticks(np.arange(len(class_names)))\n",
    "plt.title('t-SNE: ViT features - Imbalanced Dataset', fontsize=14)\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# ResNet t-SNE - Full Dataset\n",
    "plt.subplot(2, 3, 2)\n",
    "scatter = plt.scatter(resnet_tsne_full_result[:, 0], resnet_tsne_full_result[:, 1], c=resnet_labels_full, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(scatter, boundaries=np.arange(len(class_names)+1)-0.5).set_ticks(np.arange(len(class_names)))\n",
    "plt.title('t-SNE: ResNet features - Full Dataset', fontsize=14)\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# ResNet t-SNE - Imbalanced Dataset\n",
    "plt.subplot(2, 3, 5)\n",
    "scatter = plt.scatter(resnet_tsne_imbalanced_result[:, 0], resnet_tsne_imbalanced_result[:, 1], c=resnet_labels_imbalanced, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(scatter, boundaries=np.arange(len(class_names)+1)-0.5).set_ticks(np.arange(len(class_names)))\n",
    "plt.title('t-SNE: ResNet features - Imbalanced Dataset', fontsize=14)\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# MobileViT t-SNE - Full Dataset\n",
    "plt.subplot(2, 3, 3)\n",
    "scatter = plt.scatter(mobilevit_tsne_full_result[:, 0], mobilevit_tsne_full_result[:, 1], c=mobilevit_labels_full, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(scatter, boundaries=np.arange(len(class_names)+1)-0.5).set_ticks(np.arange(len(class_names)))\n",
    "plt.title('t-SNE: MobileViT features - Full Dataset', fontsize=14)\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# MobileViT t-SNE - Imbalanced Dataset\n",
    "plt.subplot(2, 3, 6)\n",
    "scatter = plt.scatter(mobilevit_tsne_imbalanced_result[:, 0], mobilevit_tsne_imbalanced_result[:, 1], c=mobilevit_labels_imbalanced, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(scatter, boundaries=np.arange(len(class_names)+1)-0.5).set_ticks(np.arange(len(class_names)))\n",
    "plt.title('t-SNE: MobileViT features - Imbalanced Dataset', fontsize=14)\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'tsne_comparison_imbalanced.png'))\n",
    "plt.close()\n",
    "\n",
    "min_class_name = class_names[min_class]\n",
    "\n",
    "def add_minority_flag(labels, min_class):\n",
    "    return ['Minority Class' if l == min_class else 'Other Classes' for l in labels]\n",
    "\n",
    "\n",
    "vit_df_full = pd.DataFrame({\n",
    "    'x': vit_tsne_full_result[:, 0],\n",
    "    'y': vit_tsne_full_result[:, 1],\n",
    "    'class': [class_names[l] for l in vit_labels_full],\n",
    "    'minority_flag': add_minority_flag(vit_labels_full, min_class)\n",
    "})\n",
    "\n",
    "resnet_df_full = pd.DataFrame({\n",
    "    'x': resnet_tsne_full_result[:, 0],\n",
    "    'y': resnet_tsne_full_result[:, 1],\n",
    "    'class': [class_names[l] for l in resnet_labels_full],\n",
    "    'minority_flag': add_minority_flag(resnet_labels_full, min_class)\n",
    "})\n",
    "\n",
    "mobilevit_df_full = pd.DataFrame({\n",
    "    'x': mobilevit_tsne_full_result[:, 0],\n",
    "    'y': mobilevit_tsne_full_result[:, 1],\n",
    "    'class': [class_names[l] for l in mobilevit_labels_full],\n",
    "    'minority_flag': add_minority_flag(mobilevit_labels_full, min_class)\n",
    "})\n",
    "\n",
    "# Imbalanced dataset\n",
    "vit_df_imbalanced = pd.DataFrame({\n",
    "    'x': vit_tsne_imbalanced_result[:, 0],\n",
    "    'y': vit_tsne_imbalanced_result[:, 1],\n",
    "    'class': [class_names[l] for l in vit_labels_imbalanced],\n",
    "    'minority_flag': add_minority_flag(vit_labels_imbalanced, min_class)\n",
    "})\n",
    "\n",
    "resnet_df_imbalanced = pd.DataFrame({\n",
    "    'x': resnet_tsne_imbalanced_result[:, 0],\n",
    "    'y': resnet_tsne_imbalanced_result[:, 1],\n",
    "    'class': [class_names[l] for l in resnet_labels_imbalanced],\n",
    "    'minority_flag': add_minority_flag(resnet_labels_imbalanced, min_class)\n",
    "})\n",
    "\n",
    "mobilevit_df_imbalanced = pd.DataFrame({\n",
    "    'x': mobilevit_tsne_imbalanced_result[:, 0],\n",
    "    'y': mobilevit_tsne_imbalanced_result[:, 1],\n",
    "    'class': [class_names[l] for l in mobilevit_labels_imbalanced],\n",
    "    'minority_flag': add_minority_flag(mobilevit_labels_imbalanced, min_class)\n",
    "})\n",
    "\n",
    "\n",
    "# ViT - Full Dataset\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.scatterplot(\n",
    "    x='x', y='y',\n",
    "    hue='class',\n",
    "    palette=sns.color_palette(\"hls\", len(class_names)),\n",
    "    data=vit_df_full,\n",
    "    legend=\"full\",\n",
    "    alpha=0.8\n",
    ")\n",
    "plt.title('t-SNE: ViT features - Full Dataset', fontsize=18)\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=14)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'vit_tsne_full_labeled.png'))\n",
    "plt.close()\n",
    "\n",
    "# ViT - Imbalanced Dataset\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.scatterplot(\n",
    "    x='x', y='y',\n",
    "    hue='class',\n",
    "    palette=sns.color_palette(\"hls\", len(class_names)),\n",
    "    data=vit_df_imbalanced,\n",
    "    legend=\"full\",\n",
    "    alpha=0.8\n",
    ")\n",
    "plt.title('t-SNE: ViT features - Imbalanced Dataset', fontsize=18)\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=14)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'vit_tsne_imbalanced_labeled.png'))\n",
    "plt.close()\n",
    "\n",
    "# ResNet - Full Dataset\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.scatterplot(\n",
    "    x='x', y='y',\n",
    "    hue='class',\n",
    "    palette=sns.color_palette(\"hls\", len(class_names)),\n",
    "    data=resnet_df_full,\n",
    "    legend=\"full\",\n",
    "    alpha=0.8\n",
    ")\n",
    "plt.title('t-SNE: ResNet features - Full Dataset', fontsize=18)\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=14)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'resnet_tsne_full_labeled.png'))\n",
    "plt.close()\n",
    "\n",
    "# ResNet - Imbalanced Dataset\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.scatterplot(\n",
    "    x='x', y='y',\n",
    "    hue='class',\n",
    "    palette=sns.color_palette(\"hls\", len(class_names)),\n",
    "    data=resnet_df_imbalanced,\n",
    "    legend=\"full\",\n",
    "    alpha=0.8\n",
    ")\n",
    "plt.title('t-SNE: ResNet features - Imbalanced Dataset', fontsize=18)\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=14)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'resnet_tsne_imbalanced_labeled.png'))\n",
    "plt.close()\n",
    "\n",
    "# MobileViT - Full Dataset\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.scatterplot(\n",
    "    x='x', y='y',\n",
    "    hue='class',\n",
    "    palette=sns.color_palette(\"hls\", len(class_names)),\n",
    "    data=mobilevit_df_full,\n",
    "    legend=\"full\",\n",
    "    alpha=0.8\n",
    ")\n",
    "plt.title('t-SNE: MobileViT features - Full Dataset', fontsize=18)\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=14)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'mobilevit_tsne_full_labeled.png'))\n",
    "plt.close()\n",
    "\n",
    "# MobileViT - Imbalanced Dataset\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.scatterplot(\n",
    "    x='x', y='y',\n",
    "    hue='class',\n",
    "    palette=sns.color_palette(\"hls\", len(class_names)),\n",
    "    data=mobilevit_df_imbalanced,\n",
    "    legend=\"full\",\n",
    "    alpha=0.8\n",
    ")\n",
    "plt.title('t-SNE: MobileViT features - Imbalanced Dataset', fontsize=18)\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=14)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'mobilevit_tsne_imbalanced_labeled.png'))\n",
    "plt.close()\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(30, 10))\n",
    "\n",
    "# ViT Full dataset\n",
    "sns.scatterplot(\n",
    "    x='x', y='y',\n",
    "    hue='minority_flag',\n",
    "    palette={'Minority Class': 'red', 'Other Classes': 'gray'},\n",
    "    data=vit_df_full,\n",
    "    alpha=0.8,\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title('ViT: Full Dataset\\nRed: Minority Class', fontsize=16)\n",
    "axes[0].set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "axes[0].set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "\n",
    "# ViT Imbalanced dataset\n",
    "sns.scatterplot(\n",
    "    x='x', y='y',\n",
    "    hue='minority_flag',\n",
    "    palette={'Minority Class': 'red', 'Other Classes': 'gray'},\n",
    "    data=vit_df_imbalanced,\n",
    "    alpha=0.8,\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title('ViT: Imbalanced Dataset\\nRed: Minority Class', fontsize=16)\n",
    "axes[1].set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "axes[1].set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "\n",
    "# MobileViT Full dataset\n",
    "sns.scatterplot(\n",
    "    x='x', y='y',\n",
    "    hue='minority_flag',\n",
    "    palette={'Minority Class': 'red', 'Other Classes': 'gray'},\n",
    "    data=mobilevit_df_full,\n",
    "    alpha=0.8,\n",
    "    ax=axes[2]\n",
    ")\n",
    "axes[2].set_title('MobileViT: Full Dataset\\nRed: Minority Class', fontsize=16)\n",
    "axes[2].set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "axes[2].set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'vit_mobilevit_minority_class_comparison.png'))\n",
    "plt.close()\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(30, 10))\n",
    "\n",
    "# ResNet Full dataset\n",
    "sns.scatterplot(\n",
    "    x='x', y='y',\n",
    "    hue='minority_flag',\n",
    "    palette={'Minority Class': 'red', 'Other Classes': 'gray'},\n",
    "    data=resnet_df_full,\n",
    "    alpha=0.8,\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title('ResNet: Full Dataset\\nRed: Minority Class', fontsize=16)\n",
    "axes[0].set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "axes[0].set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "\n",
    "# ResNet Imbalanced dataset\n",
    "sns.scatterplot(\n",
    "    x='x', y='y',\n",
    "    hue='minority_flag',\n",
    "    palette={'Minority Class': 'red', 'Other Classes': 'gray'},\n",
    "    data=resnet_df_imbalanced,\n",
    "    alpha=0.8,\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title('ResNet: Imbalanced Dataset\\nRed: Minority Class', fontsize=16)\n",
    "axes[1].set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "axes[1].set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "\n",
    "# MobileViT Imbalanced dataset\n",
    "sns.scatterplot(\n",
    "    x='x', y='y',\n",
    "    hue='minority_flag',\n",
    "    palette={'Minority Class': 'red', 'Other Classes': 'gray'},\n",
    "    data=mobilevit_df_imbalanced,\n",
    "    alpha=0.8,\n",
    "    ax=axes[2]\n",
    ")\n",
    "axes[2].set_title('MobileViT: Imbalanced Dataset\\nRed: Minority Class', fontsize=16)\n",
    "axes[2].set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "axes[2].set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'resnet_mobilevit_minority_class_comparison.png'))\n",
    "plt.close()\n",
    "\n",
    "metrics = {}\n",
    "\n",
    "# Full dataset metrics\n",
    "metrics['vit_full_pca'] = calculate_separation_metrics(vit_pca_full_result, vit_labels_full)\n",
    "metrics['resnet_full_pca'] = calculate_separation_metrics(resnet_pca_full_result, resnet_labels_full)\n",
    "metrics['mobilevit_full_pca'] = calculate_separation_metrics(mobilevit_pca_full_result, mobilevit_labels_full)\n",
    "metrics['vit_full_tsne'] = calculate_separation_metrics(vit_tsne_full_result, vit_labels_full)\n",
    "metrics['resnet_full_tsne'] = calculate_separation_metrics(resnet_tsne_full_result, resnet_labels_full)\n",
    "metrics['mobilevit_full_tsne'] = calculate_separation_metrics(mobilevit_tsne_full_result, mobilevit_labels_full)\n",
    "\n",
    "# Imbalanced dataset metrics\n",
    "metrics['vit_imbalanced_pca'] = calculate_separation_metrics(vit_pca_imbalanced_result, vit_labels_imbalanced)\n",
    "metrics['resnet_imbalanced_pca'] = calculate_separation_metrics(resnet_pca_imbalanced_result, resnet_labels_imbalanced)\n",
    "metrics['mobilevit_imbalanced_pca'] = calculate_separation_metrics(mobilevit_pca_imbalanced_result, mobilevit_labels_imbalanced)\n",
    "metrics['vit_imbalanced_tsne'] = calculate_separation_metrics(vit_tsne_imbalanced_result, vit_labels_imbalanced)\n",
    "metrics['resnet_imbalanced_tsne'] = calculate_separation_metrics(resnet_tsne_imbalanced_result, resnet_labels_imbalanced)\n",
    "metrics['mobilevit_imbalanced_tsne'] = calculate_separation_metrics(mobilevit_tsne_imbalanced_result, mobilevit_labels_imbalanced)\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Model': ['ViT (Full, PCA)', 'ResNet (Full, PCA)', 'MobileViT (Full, PCA)', \n",
    "              'ViT (Full, t-SNE)', 'ResNet (Full, t-SNE)', 'MobileViT (Full, t-SNE)',\n",
    "              'ViT (Imbalanced, PCA)', 'ResNet (Imbalanced, PCA)', 'MobileViT (Imbalanced, PCA)',\n",
    "              'ViT (Imbalanced, t-SNE)', 'ResNet (Imbalanced, t-SNE)', 'MobileViT (Imbalanced, t-SNE)'],\n",
    "    'Intra-class Distance': [\n",
    "        metrics['vit_full_pca']['avg_intra_class_distance'],\n",
    "        metrics['resnet_full_pca']['avg_intra_class_distance'],\n",
    "        metrics['mobilevit_full_pca']['avg_intra_class_distance'],\n",
    "        metrics['vit_full_tsne']['avg_intra_class_distance'],\n",
    "        metrics['resnet_full_tsne']['avg_intra_class_distance'],\n",
    "        metrics['mobilevit_full_tsne']['avg_intra_class_distance'],\n",
    "        metrics['vit_imbalanced_pca']['avg_intra_class_distance'],\n",
    "        metrics['resnet_imbalanced_pca']['avg_intra_class_distance'],\n",
    "        metrics['mobilevit_imbalanced_pca']['avg_intra_class_distance'],\n",
    "        metrics['vit_imbalanced_tsne']['avg_intra_class_distance'],\n",
    "        metrics['resnet_imbalanced_tsne']['avg_intra_class_distance'],\n",
    "        metrics['mobilevit_imbalanced_tsne']['avg_intra_class_distance']\n",
    "    ],\n",
    "    'Inter-class Distance': [\n",
    "        metrics['vit_full_pca']['avg_inter_class_distance'],\n",
    "        metrics['resnet_full_pca']['avg_inter_class_distance'],\n",
    "        metrics['mobilevit_full_pca']['avg_inter_class_distance'],\n",
    "        metrics['vit_full_tsne']['avg_inter_class_distance'],\n",
    "        metrics['resnet_full_tsne']['avg_inter_class_distance'],\n",
    "        metrics['mobilevit_full_tsne']['avg_inter_class_distance'],\n",
    "        metrics['vit_imbalanced_pca']['avg_inter_class_distance'],\n",
    "        metrics['resnet_imbalanced_pca']['avg_inter_class_distance'],\n",
    "        metrics['mobilevit_imbalanced_pca']['avg_inter_class_distance'],\n",
    "        metrics['vit_imbalanced_tsne']['avg_inter_class_distance'],\n",
    "        metrics['resnet_imbalanced_tsne']['avg_inter_class_distance'],\n",
    "        metrics['mobilevit_imbalanced_tsne']['avg_inter_class_distance']\n",
    "    ],\n",
    "    'Separation Ratio': [\n",
    "        metrics['vit_full_pca']['separation_ratio'],\n",
    "        metrics['resnet_full_pca']['separation_ratio'],\n",
    "        metrics['mobilevit_full_pca']['separation_ratio'],\n",
    "        metrics['vit_full_tsne']['separation_ratio'],\n",
    "        metrics['resnet_full_tsne']['separation_ratio'],\n",
    "        metrics['mobilevit_full_tsne']['separation_ratio'],\n",
    "        metrics['vit_imbalanced_pca']['separation_ratio'],\n",
    "        metrics['resnet_imbalanced_pca']['separation_ratio'],\n",
    "        metrics['mobilevit_imbalanced_pca']['separation_ratio'],\n",
    "        metrics['vit_imbalanced_tsne']['separation_ratio'],\n",
    "        metrics['resnet_imbalanced_tsne']['separation_ratio'],\n",
    "        metrics['mobilevit_imbalanced_tsne']['separation_ratio']\n",
    "    ]\n",
    "})\n",
    "\n",
    "\n",
    "# print(\"\\nFeature Separation Metrics:\")\n",
    "# print(metrics_df)\n",
    "\n",
    "metrics_df.to_csv(os.path.join(output_dir, 'imbalance_feature_separation_metrics.csv'), index=False)\n",
    "print(f\"Saved feature separation metrics to {os.path.join(output_dir, 'imbalance_feature_separation_metrics.csv')}\")\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "metrics_df_plot = pd.melt(metrics_df, id_vars=['Model'], \n",
    "                          value_vars=['Intra-class Distance', 'Inter-class Distance', 'Separation Ratio'],\n",
    "                          var_name='Metric', value_name='Value')\n",
    "\n",
    "ax = sns.barplot(x='Model', y='Value', hue='Metric', data=metrics_df_plot)\n",
    "plt.title('Feature Space Separation Metrics Comparison', fontsize=18)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'imbalance_separation_metrics_comparison.png'))\n",
    "plt.close()\n",
    "\n",
    "vit_metrics = metrics_df[metrics_df['Model'].str.contains('ViT(?! Mobile)')]\n",
    "resnet_metrics = metrics_df[metrics_df['Model'].str.contains('ResNet')]\n",
    "mobilevit_metrics = metrics_df[metrics_df['Model'].str.contains('MobileViT')]\n",
    "pca_metrics = metrics_df[metrics_df['Model'].str.contains('PCA')]\n",
    "tsne_metrics = metrics_df[metrics_df['Model'].str.contains('t-SNE')]\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 24))\n",
    "\n",
    "vit_metrics_plot = pd.melt(vit_metrics, id_vars=['Model'], \n",
    "                          value_vars=['Intra-class Distance', 'Inter-class Distance', 'Separation Ratio'],\n",
    "                          var_name='Metric', value_name='Value')\n",
    "sns.barplot(x='Model', y='Value', hue='Metric', data=vit_metrics_plot, ax=axes[0])\n",
    "axes[0].set_title('ViT Feature Space Metrics: Full vs Imbalanced', fontsize=16)\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45, ha='right')\n",
    "axes[0].legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "resnet_metrics_plot = pd.melt(resnet_metrics, id_vars=['Model'], \n",
    "                            value_vars=['Intra-class Distance', 'Inter-class Distance', 'Separation Ratio'],\n",
    "                            var_name='Metric', value_name='Value')\n",
    "sns.barplot(x='Model', y='Value', hue='Metric', data=resnet_metrics_plot, ax=axes[1])\n",
    "axes[1].set_title('ResNet Feature Space Metrics: Full vs Imbalanced', fontsize=16)\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45, ha='right')\n",
    "axes[1].legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "mobilevit_metrics_plot = pd.melt(mobilevit_metrics, id_vars=['Model'], \n",
    "                                value_vars=['Intra-class Distance', 'Inter-class Distance', 'Separation Ratio'],\n",
    "                                var_name='Metric', value_name='Value')\n",
    "sns.barplot(x='Model', y='Value', hue='Metric', data=mobilevit_metrics_plot, ax=axes[2])\n",
    "axes[2].set_title('MobileViT Feature Space Metrics: Full vs Imbalanced', fontsize=16)\n",
    "axes[2].set_xticklabels(axes[2].get_xticklabels(), rotation=45, ha='right')\n",
    "axes[2].legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'model_comparison_metrics.png'))\n",
    "plt.close()\n",
    "\n",
    "def calculate_minority_class_metrics(features, labels, min_class):\n",
    "    \"\"\"\n",
    "    Calculate metrics specific to the minority class:\n",
    "    - Distance from minority class center to other class centers\n",
    "    - Compactness of minority class (average distance from points to center)\n",
    "    \"\"\"\n",
    "    all_classes = np.unique(labels)\n",
    "    \n",
    "    \n",
    "    centers = {}\n",
    "    for c in all_classes:\n",
    "        centers[c] = np.mean(features[labels == c], axis=0)\n",
    "    \n",
    "    \n",
    "    min_features = features[labels == min_class]\n",
    "    min_center = centers[min_class]\n",
    "    min_distances = np.sqrt(np.sum((min_features - min_center)**2, axis=1))\n",
    "    min_compactness = np.mean(min_distances) if len(min_distances) > 0 else 0.0\n",
    "    \n",
    "    \n",
    "    distances_to_other_centers = []\n",
    "    for c in all_classes:\n",
    "        if c != min_class:\n",
    "            dist = np.sqrt(np.sum((centers[min_class] - centers[c])**2))\n",
    "            distances_to_other_centers.append(dist)\n",
    "    \n",
    "    avg_distance_to_others = np.mean(distances_to_other_centers) if distances_to_other_centers else 0.0\n",
    "    min_separation = avg_distance_to_others / min_compactness if min_compactness != 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'minority_compactness': min_compactness,\n",
    "        'avg_distance_to_others': avg_distance_to_others,\n",
    "        'minority_separation_ratio': min_separation\n",
    "    }\n",
    "\n",
    "minority_metrics = {}\n",
    "\n",
    "# Full dataset\n",
    "minority_metrics['vit_full_pca'] = calculate_minority_class_metrics(vit_pca_full_result, vit_labels_full, min_class)\n",
    "minority_metrics['resnet_full_pca'] = calculate_minority_class_metrics(resnet_pca_full_result, resnet_labels_full, min_class)\n",
    "minority_metrics['mobilevit_full_pca'] = calculate_minority_class_metrics(mobilevit_pca_full_result, mobilevit_labels_full, min_class)\n",
    "minority_metrics['vit_full_tsne'] = calculate_minority_class_metrics(vit_tsne_full_result, vit_labels_full, min_class)\n",
    "minority_metrics['resnet_full_tsne'] = calculate_minority_class_metrics(resnet_tsne_full_result, resnet_labels_full, min_class)\n",
    "minority_metrics['mobilevit_full_tsne'] = calculate_minority_class_metrics(mobilevit_tsne_full_result, mobilevit_labels_full, min_class)\n",
    "\n",
    "# Imbalanced dataset\n",
    "minority_metrics['vit_imbalanced_pca'] = calculate_minority_class_metrics(vit_pca_imbalanced_result, vit_labels_imbalanced, min_class)\n",
    "minority_metrics['resnet_imbalanced_pca'] = calculate_minority_class_metrics(resnet_pca_imbalanced_result, resnet_labels_imbalanced, min_class)\n",
    "minority_metrics['mobilevit_imbalanced_pca'] = calculate_minority_class_metrics(mobilevit_pca_imbalanced_result, mobilevit_labels_imbalanced, min_class)\n",
    "minority_metrics['vit_imbalanced_tsne'] = calculate_minority_class_metrics(vit_tsne_imbalanced_result, vit_labels_imbalanced, min_class)\n",
    "minority_metrics['resnet_imbalanced_tsne'] = calculate_minority_class_metrics(resnet_tsne_imbalanced_result, resnet_labels_imbalanced, min_class)\n",
    "minority_metrics['mobilevit_imbalanced_tsne'] = calculate_minority_class_metrics(mobilevit_tsne_imbalanced_result, mobilevit_labels_imbalanced, min_class)\n",
    "\n",
    "\n",
    "minority_metrics_df = pd.DataFrame({\n",
    "    'Model': ['ViT (Full, PCA)', 'ResNet (Full, PCA)', 'MobileViT (Full, PCA)', \n",
    "              'ViT (Full, t-SNE)', 'ResNet (Full, t-SNE)', 'MobileViT (Full, t-SNE)',\n",
    "              'ViT (Imbalanced, PCA)', 'ResNet (Imbalanced, PCA)', 'MobileViT (Imbalanced, PCA)',\n",
    "              'ViT (Imbalanced, t-SNE)', 'ResNet (Imbalanced, t-SNE)', 'MobileViT (Imbalanced, t-SNE)'],\n",
    "    'Minority Compactness': [\n",
    "        minority_metrics['vit_full_pca']['minority_compactness'],\n",
    "        minority_metrics['resnet_full_pca']['minority_compactness'],\n",
    "        minority_metrics['mobilevit_full_pca']['minority_compactness'],\n",
    "        minority_metrics['vit_full_tsne']['minority_compactness'],\n",
    "        minority_metrics['resnet_full_tsne']['minority_compactness'],\n",
    "        minority_metrics['mobilevit_full_tsne']['minority_compactness'],\n",
    "        minority_metrics['vit_imbalanced_pca']['minority_compactness'],\n",
    "        minority_metrics['resnet_imbalanced_pca']['minority_compactness'],\n",
    "        minority_metrics['mobilevit_imbalanced_pca']['minority_compactness'],\n",
    "        minority_metrics['vit_imbalanced_tsne']['minority_compactness'],\n",
    "        minority_metrics['resnet_imbalanced_tsne']['minority_compactness'],\n",
    "        minority_metrics['mobilevit_imbalanced_tsne']['minority_compactness']\n",
    "    ],\n",
    "    'Avg Distance to Others': [\n",
    "        minority_metrics['vit_full_pca']['avg_distance_to_others'],\n",
    "        minority_metrics['resnet_full_pca']['avg_distance_to_others'],\n",
    "        minority_metrics['mobilevit_full_pca']['avg_distance_to_others'],\n",
    "        minority_metrics['vit_full_tsne']['avg_distance_to_others'],\n",
    "        minority_metrics['resnet_full_tsne']['avg_distance_to_others'],\n",
    "        minority_metrics['mobilevit_full_tsne']['avg_distance_to_others'],\n",
    "        minority_metrics['vit_imbalanced_pca']['avg_distance_to_others'],\n",
    "        minority_metrics['resnet_imbalanced_pca']['avg_distance_to_others'],\n",
    "        minority_metrics['mobilevit_imbalanced_pca']['avg_distance_to_others'],\n",
    "        minority_metrics['vit_imbalanced_tsne']['avg_distance_to_others'],\n",
    "        minority_metrics['resnet_imbalanced_tsne']['avg_distance_to_others'],\n",
    "        minority_metrics['mobilevit_imbalanced_tsne']['avg_distance_to_others']\n",
    "    ],\n",
    "    'Minority Separation Ratio': [\n",
    "        minority_metrics['vit_full_pca']['minority_separation_ratio'],\n",
    "        minority_metrics['resnet_full_pca']['minority_separation_ratio'],\n",
    "        minority_metrics['mobilevit_full_pca']['minority_separation_ratio'],\n",
    "        minority_metrics['vit_full_tsne']['minority_separation_ratio'],\n",
    "        minority_metrics['resnet_full_tsne']['minority_separation_ratio'],\n",
    "        minority_metrics['mobilevit_full_tsne']['minority_separation_ratio'],\n",
    "        minority_metrics['vit_imbalanced_pca']['minority_separation_ratio'],\n",
    "        minority_metrics['resnet_imbalanced_pca']['minority_separation_ratio'],\n",
    "        minority_metrics['mobilevit_imbalanced_pca']['minority_separation_ratio'],\n",
    "        minority_metrics['vit_imbalanced_tsne']['minority_separation_ratio'],\n",
    "        minority_metrics['resnet_imbalanced_tsne']['minority_separation_ratio'],\n",
    "        minority_metrics['mobilevit_imbalanced_tsne']['minority_separation_ratio']\n",
    "    ]\n",
    "})\n",
    "\n",
    "# print(\"\\nMinority Class Metrics:\")\n",
    "# print(minority_metrics_df)\n",
    "\n",
    "minority_metrics_df.to_csv(os.path.join(output_dir, 'minority_class_metrics.csv'), index=False)\n",
    "print(f\"Saved minority class metrics to {os.path.join(output_dir, 'minority_class_metrics.csv')}\")\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "minority_metrics_plot = pd.melt(minority_metrics_df, id_vars=['Model'], \n",
    "                              value_vars=['Minority Compactness', 'Avg Distance to Others', 'Minority Separation Ratio'],\n",
    "                              var_name='Metric', value_name='Value')\n",
    "\n",
    "sns.barplot(x='Model', y='Value', hue='Metric', data=minority_metrics_plot)\n",
    "plt.title('Minority Class Metrics Comparison', fontsize=18)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'minority_class_metrics_comparison.png'))\n",
    "plt.close()\n",
    "\n",
    "imbalance_impact = pd.DataFrame({\n",
    "    'Model': ['ViT (PCA)', 'ResNet (PCA)', 'MobileViT (PCA)', 'ViT (t-SNE)', 'ResNet (t-SNE)', 'MobileViT (t-SNE)'],\n",
    "    'Separation Ratio Change (%)': [\n",
    "        (metrics['vit_imbalanced_pca']['separation_ratio'] / metrics['vit_full_pca']['separation_ratio'] - 1) * 100,\n",
    "        (metrics['resnet_imbalanced_pca']['separation_ratio'] / metrics['resnet_full_pca']['separation_ratio'] - 1) * 100,\n",
    "        (metrics['mobilevit_imbalanced_pca']['separation_ratio'] / metrics['mobilevit_full_pca']['separation_ratio'] - 1) * 100,\n",
    "        (metrics['vit_imbalanced_tsne']['separation_ratio'] / metrics['vit_full_tsne']['separation_ratio'] - 1) * 100,\n",
    "        (metrics['resnet_imbalanced_tsne']['separation_ratio'] / metrics['resnet_full_tsne']['separation_ratio'] - 1) * 100,\n",
    "        (metrics['mobilevit_imbalanced_tsne']['separation_ratio'] / metrics['mobilevit_full_tsne']['separation_ratio'] - 1) * 100\n",
    "    ],\n",
    "    'Minority Separation Change (%)': [\n",
    "        (minority_metrics['vit_imbalanced_pca']['minority_separation_ratio'] / \n",
    "         minority_metrics['vit_full_pca']['minority_separation_ratio'] - 1) * 100 if minority_metrics['vit_full_pca']['minority_separation_ratio'] != 0 else 0.0,\n",
    "        (minority_metrics['resnet_imbalanced_pca']['minority_separation_ratio'] / \n",
    "         minority_metrics['resnet_full_pca']['minority_separation_ratio'] - 1) * 100 if minority_metrics['resnet_full_pca']['minority_separation_ratio'] != 0 else 0.0,\n",
    "        (minority_metrics['mobilevit_imbalanced_pca']['minority_separation_ratio'] / \n",
    "         minority_metrics['mobilevit_full_pca']['minority_separation_ratio'] - 1) * 100 if minority_metrics['mobilevit_full_pca']['minority_separation_ratio'] != 0 else 0.0,\n",
    "        (minority_metrics['vit_imbalanced_tsne']['minority_separation_ratio'] / \n",
    "         minority_metrics['vit_full_tsne']['minority_separation_ratio'] - 1) * 100 if minority_metrics['vit_full_tsne']['minority_separation_ratio'] != 0 else 0.0,\n",
    "        (minority_metrics['resnet_imbalanced_tsne']['minority_separation_ratio'] / \n",
    "         minority_metrics['resnet_full_tsne']['minority_separation_ratio'] - 1) * 100 if minority_metrics['resnet_full_tsne']['minority_separation_ratio'] != 0 else 0.0,\n",
    "        (minority_metrics['mobilevit_imbalanced_tsne']['minority_separation_ratio'] / \n",
    "         minority_metrics['mobilevit_full_tsne']['minority_separation_ratio'] - 1) * 100 if minority_metrics['mobilevit_full_tsne']['minority_separation_ratio'] != 0 else 0.0\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\nImbalance Impact (% change from full to imbalanced dataset):\")\n",
    "print(imbalance_impact)\n",
    "\n",
    "imbalance_impact.to_csv(os.path.join(output_dir, 'imbalance_impact.csv'), index=False)\n",
    "print(f\"Saved imbalance impact analysis to {os.path.join(output_dir, 'imbalance_impact.csv')}\")\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "imbalance_impact_plot = pd.melt(imbalance_impact, id_vars=['Model'], \n",
    "                              value_vars=['Separation Ratio Change (%)', 'Minority Separation Change (%)'],\n",
    "                              var_name='Metric', value_name='Percent Change')\n",
    "\n",
    "sns.barplot(x='Model', y='Percent Change', hue='Metric', data=imbalance_impact_plot)\n",
    "plt.title('Impact of Class Imbalance on Feature Space Metrics', fontsize=18)\n",
    "plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
    "plt.ylabel('Percent Change (%)')\n",
    "plt.legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'imbalance_impact.png'))\n",
    "plt.close()\n",
    "\n",
    "model_comparison = pd.DataFrame({\n",
    "    'Technique': ['PCA', 't-SNE'],\n",
    "    'ViT Overall Separation Ratio': [\n",
    "        metrics['vit_imbalanced_pca']['separation_ratio'],\n",
    "        metrics['vit_imbalanced_tsne']['separation_ratio']\n",
    "    ],\n",
    "    'ResNet Overall Separation Ratio': [\n",
    "        metrics['resnet_imbalanced_pca']['separation_ratio'],\n",
    "        metrics['resnet_imbalanced_tsne']['separation_ratio']\n",
    "    ],\n",
    "    'MobileViT Overall Separation Ratio': [\n",
    "        metrics['mobilevit_imbalanced_pca']['separation_ratio'],\n",
    "        metrics['mobilevit_imbalanced_tsne']['separation_ratio']\n",
    "    ],\n",
    "    'ViT Minority Separation Ratio': [\n",
    "        minority_metrics['vit_imbalanced_pca']['minority_separation_ratio'],\n",
    "        minority_metrics['vit_imbalanced_tsne']['minority_separation_ratio']\n",
    "    ],\n",
    "    'ResNet Minority Separation Ratio': [\n",
    "        minority_metrics['resnet_imbalanced_pca']['minority_separation_ratio'],\n",
    "        minority_metrics['resnet_imbalanced_tsne']['minority_separation_ratio']\n",
    "    ],\n",
    "    'MobileViT Minority Separation Ratio': [\n",
    "        minority_metrics['mobilevit_imbalanced_pca']['minority_separation_ratio'],\n",
    "        minority_metrics['mobilevit_imbalanced_tsne']['minority_separation_ratio']\n",
    "    ],\n",
    "    'ViT Imbalance Impact (%)': [\n",
    "        (metrics['vit_imbalanced_pca']['separation_ratio'] / metrics['vit_full_pca']['separation_ratio'] - 1) * 100,\n",
    "        (metrics['vit_imbalanced_tsne']['separation_ratio'] / metrics['vit_full_tsne']['separation_ratio'] - 1) * 100\n",
    "    ],\n",
    "    'ResNet Imbalance Impact (%)': [\n",
    "        (metrics['resnet_imbalanced_pca']['separation_ratio'] / metrics['resnet_full_pca']['separation_ratio'] - 1) * 100,\n",
    "        (metrics['resnet_imbalanced_tsne']['separation_ratio'] / metrics['resnet_full_tsne']['separation_ratio'] - 1) * 100\n",
    "    ],\n",
    "    'MobileViT Imbalance Impact (%)': [\n",
    "        (metrics['mobilevit_imbalanced_pca']['separation_ratio'] / metrics['mobilevit_full_pca']['separation_ratio'] - 1) * 100,\n",
    "        (metrics['mobilevit_imbalanced_tsne']['separation_ratio'] / metrics['mobilevit_full_tsne']['separation_ratio'] - 1) * 100\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\nDirectly Comparing ViT, ResNet, and MobileViT on Imbalanced Data:\")\n",
    "print(model_comparison)\n",
    "\n",
    "model_comparison.to_csv(os.path.join(output_dir, 'vit_resnet_mobilevit_imbalance.csv'), index=False)\n",
    "print(f\"Saved ViT, ResNet, and MobileViT comparison to {os.path.join(output_dir, 'vit_resnet_mobilevit_imbalance.csv')}\")\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(x='Technique', y='value', hue='variable', \n",
    "           data=pd.melt(model_comparison, id_vars=['Technique'], \n",
    "                      value_vars=['ViT Imbalance Impact (%)', 'ResNet Imbalance Impact (%)', 'MobileViT Imbalance Impact (%)']))\n",
    "plt.title('ViT vs ResNet vs MobileViT: Impact of Class Imbalance on Separation', fontsize=18)\n",
    "plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
    "plt.ylabel('Percent Change in Separation Ratio (%)')\n",
    "plt.legend(title='Model')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'vit_resnet_mobilevit_imbalance_impact.png'))\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(18, 16))\n",
    "\n",
    "comprehensive_df = pd.DataFrame({\n",
    "    'Model': ['ViT', 'ResNet', 'MobileViT', 'ViT', 'ResNet', 'MobileViT'],\n",
    "    'Dataset': ['Full', 'Full', 'Full', 'Imbalanced', 'Imbalanced', 'Imbalanced'],\n",
    "    'Overall Separation (PCA)': [\n",
    "        metrics['vit_full_pca']['separation_ratio'],\n",
    "        metrics['resnet_full_pca']['separation_ratio'],\n",
    "        metrics['mobilevit_full_pca']['separation_ratio'],\n",
    "        metrics['vit_imbalanced_pca']['separation_ratio'],\n",
    "        metrics['resnet_imbalanced_pca']['separation_ratio'],\n",
    "        metrics['mobilevit_imbalanced_pca']['separation_ratio']\n",
    "    ],\n",
    "    'Overall Separation (t-SNE)': [\n",
    "        metrics['vit_full_tsne']['separation_ratio'],\n",
    "        metrics['resnet_full_tsne']['separation_ratio'],\n",
    "        metrics['mobilevit_full_tsne']['separation_ratio'],\n",
    "        metrics['vit_imbalanced_tsne']['separation_ratio'],\n",
    "        metrics['resnet_imbalanced_tsne']['separation_ratio'],\n",
    "        metrics['mobilevit_imbalanced_tsne']['separation_ratio']\n",
    "    ],\n",
    "    'Minority Separation (PCA)': [\n",
    "        minority_metrics['vit_full_pca']['minority_separation_ratio'],\n",
    "        minority_metrics['resnet_full_pca']['minority_separation_ratio'],\n",
    "        minority_metrics['mobilevit_full_pca']['minority_separation_ratio'],\n",
    "        minority_metrics['vit_imbalanced_pca']['minority_separation_ratio'],\n",
    "        minority_metrics['resnet_imbalanced_pca']['minority_separation_ratio'],\n",
    "        minority_metrics['mobilevit_imbalanced_pca']['minority_separation_ratio']\n",
    "    ],\n",
    "    'Minority Separation (t-SNE)': [\n",
    "        minority_metrics['vit_full_tsne']['minority_separation_ratio'],\n",
    "        minority_metrics['resnet_full_tsne']['minority_separation_ratio'],\n",
    "        minority_metrics['mobilevit_full_tsne']['minority_separation_ratio'],\n",
    "        minority_metrics['vit_imbalanced_tsne']['minority_separation_ratio'],\n",
    "        minority_metrics['resnet_imbalanced_tsne']['minority_separation_ratio'],\n",
    "        minority_metrics['mobilevit_imbalanced_tsne']['minority_separation_ratio']\n",
    "    ]\n",
    "})\n",
    "\n",
    "plot_data = pd.melt(comprehensive_df, \n",
    "                  id_vars=['Model', 'Dataset'], \n",
    "                  value_vars=['Overall Separation (PCA)', 'Overall Separation (t-SNE)', \n",
    "                            'Minority Separation (PCA)', 'Minority Separation (t-SNE)'],\n",
    "                  var_name='Metric', value_name='Value')\n",
    "\n",
    "g = sns.catplot(\n",
    "    data=plot_data, kind=\"bar\",\n",
    "    x=\"Dataset\", y=\"Value\", hue=\"Model\", col=\"Metric\",\n",
    "    height=5, aspect=0.8, sharey=False, col_wrap=2\n",
    ")\n",
    "\n",
    "g.fig.suptitle('Comprehensive Feature Space Analysis: ViT vs ResNet vs MobileViT', fontsize=20, y=1.05)\n",
    "g.set_titles(col_template=\"{col_name}\")\n",
    "g.set_axis_labels(\"Dataset\", \"Separation Ratio\")\n",
    "g.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'comprehensive_comparison.png'))\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\nAll visualizations and analyses saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6e5916",
   "metadata": {},
   "source": [
    "# Interactive Gradcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b4528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_bundle_path = \"expt_1/vit_tiny_model_bundle_focal.pth\"\n",
    "resnet_bundle_path = \"expt_1/resnet50_model_bundle_focal.pth\"\n",
    "mobilevit_bundle_path = \"expt_1/mobilevit_xs_model_bundle.pth\"\n",
    "data_dir = \"NSD/test\"\n",
    "output_dir = \"gradcam_visualization\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "class ResNetGradCAM:\n",
    "    def __init__(self, model, target_layer_name=\"layer4\"):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        \n",
    "        \n",
    "        if target_layer_name == \"layer4\":\n",
    "            self.target_layer = model.layer4\n",
    "        else:\n",
    "            raise ValueError(f\"Target layer {target_layer_name} not found in model\")\n",
    "        \n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        \n",
    "        \n",
    "        self.hooks = []\n",
    "        self.register_hooks()\n",
    "        \n",
    "    def register_hooks(self):\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output\n",
    "            \n",
    "        def backward_hook(module, grad_input, grad_output):\n",
    "            self.gradients = grad_output[0]\n",
    "        \n",
    "        self.hooks.append(self.target_layer.register_forward_hook(forward_hook))\n",
    "        self.hooks.append(self.target_layer.register_full_backward_hook(backward_hook))\n",
    "        \n",
    "    def remove_hooks(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "        \n",
    "    def generate_cam(self, input_image, target_class=None):\n",
    "        model_output = self.model(input_image)\n",
    "        \n",
    "        if target_class is None:\n",
    "            target_class = torch.argmax(model_output, dim=1).item()\n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        one_hot = torch.zeros_like(model_output)\n",
    "        one_hot[0, target_class] = 1\n",
    "        model_output.backward(gradient=one_hot, retain_graph=True)\n",
    "        \n",
    "        gradients = self.gradients.detach().cpu().numpy()[0]\n",
    "        activations = self.activations.detach().cpu().numpy()[0]\n",
    "        weights = np.mean(gradients, axis=(1, 2))\n",
    "        \n",
    "        cam = np.zeros(activations.shape[1:], dtype=np.float32)\n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * activations[i]\n",
    "        \n",
    "        cam = np.maximum(cam, 0)\n",
    "        if np.max(cam) > 0:\n",
    "            cam = cam / np.max(cam)\n",
    "        \n",
    "        cam = cv2.resize(cam, (input_image.shape[3], input_image.shape[2]))\n",
    "        return cam, target_class\n",
    "\n",
    "class ViTGradCAM:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.target_layer = model.blocks[-1].norm1\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        self.hooks = []\n",
    "        self.register_hooks()\n",
    "        \n",
    "    def register_hooks(self):\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output\n",
    "            \n",
    "        def backward_hook(module, grad_input, grad_output):\n",
    "            self.gradients = grad_output[0]\n",
    "        \n",
    "        self.hooks.append(self.target_layer.register_forward_hook(forward_hook))\n",
    "        self.hooks.append(self.target_layer.register_full_backward_hook(backward_hook))\n",
    "        \n",
    "    def remove_hooks(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "        \n",
    "    def generate_cam(self, input_image, target_class=None):\n",
    "        model_output = self.model(input_image)\n",
    "        \n",
    "        if target_class is None:\n",
    "            target_class = torch.argmax(model_output, dim=1).item()\n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        one_hot = torch.zeros_like(model_output)\n",
    "        one_hot[0, target_class] = 1\n",
    "        model_output.backward(gradient=one_hot, retain_graph=True)\n",
    "        \n",
    "        gradients = self.gradients.detach().cpu().numpy()[0]\n",
    "        activations = self.activations.detach().cpu().numpy()[0]\n",
    "        gradients = gradients[1:]\n",
    "        activations = activations[1:]\n",
    "        weights = np.mean(gradients, axis=1)\n",
    "        cam = np.mean(activations, axis=1) * weights\n",
    "        \n",
    "        patch_size = 16\n",
    "        num_patches = int(np.sqrt(cam.shape[0]))\n",
    "        cam = cam.reshape(num_patches, num_patches)\n",
    "        cam = np.maximum(cam, 0)\n",
    "        if np.max(cam) > 0:\n",
    "            cam = cam / np.max(cam)\n",
    "        \n",
    "        cam = cv2.resize(cam, (input_image.shape[3], input_image.shape[2]))\n",
    "        return cam, target_class\n",
    "\n",
    "class MobileViTGradCAM:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.target_layer = model.stages[-1]\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        self.hooks = []\n",
    "        self.register_hooks()\n",
    "        \n",
    "    def register_hooks(self):\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output\n",
    "            \n",
    "        def backward_hook(module, grad_input, grad_output):\n",
    "            self.gradients = grad_output[0]\n",
    "        \n",
    "        self.hooks.append(self.target_layer.register_forward_hook(forward_hook))\n",
    "        self.hooks.append(self.target_layer.register_full_backward_hook(backward_hook))\n",
    "        \n",
    "    def remove_hooks(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "        \n",
    "    def generate_cam(self, input_image, target_class=None):\n",
    "        model_output = self.model(input_image)\n",
    "        \n",
    "        if target_class is None:\n",
    "            target_class = torch.argmax(model_output, dim=1).item()\n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        one_hot = torch.zeros_like(model_output)\n",
    "        one_hot[0, target_class] = 1\n",
    "        model_output.backward(gradient=one_hot, retain_graph=True)\n",
    "        \n",
    "        gradients = self.gradients.detach().cpu().numpy()[0]\n",
    "        activations = self.activations.detach().cpu().numpy()[0]\n",
    "        weights = np.mean(gradients, axis=(1, 2))\n",
    "        \n",
    "        cam = np.zeros(activations.shape[1:], dtype=np.float32)\n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * activations[i]\n",
    "        \n",
    "        cam = np.maximum(cam, 0)\n",
    "        if np.max(cam) > 0:\n",
    "            cam = cam / np.max(cam)\n",
    "        \n",
    "        cam = cv2.resize(cam, (input_image.shape[3], input_image.shape[2]))\n",
    "        return cam, target_class\n",
    "\n",
    "def load_model_bundle(bundle_path):\n",
    "    \"\"\"\n",
    "    Load a model bundle and return the model, transform, and model type\n",
    "    \"\"\"\n",
    "    bundle = torch.load(bundle_path, map_location=\"cpu\", weights_only=False)\n",
    "    model_name = bundle[\"model_name\"]\n",
    "    \n",
    "    if \"vit\" in model_name and \"mobilevit\" not in model_name:\n",
    "        model = timm.create_model(model_name, pretrained=False, num_classes=bundle[\"num_classes\"])\n",
    "        model_type = \"vit\"\n",
    "    elif \"resnet\" in model_name:\n",
    "        model = models.resnet50(pretrained=False)\n",
    "        model.fc = nn.Linear(model.fc.in_features, bundle[\"num_classes\"])\n",
    "        model_type = \"resnet\"\n",
    "    elif \"mobilevit\" in model_name:\n",
    "        model = timm.create_model(model_name, pretrained=False, num_classes=bundle[\"num_classes\"])\n",
    "        model_type = \"mobilevit\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_name}\")\n",
    "    \n",
    "    model.load_state_dict(bundle[\"state_dict\"])\n",
    "    model.eval()\n",
    "    \n",
    "    return model, bundle[\"transform\"], model_type\n",
    "\n",
    "def show_cam_on_image(img, mask, alpha=0.5):\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
    "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "    img_uint8 = np.uint8(255 * img)\n",
    "    cam_img = heatmap * alpha + img_uint8 * (1 - alpha)\n",
    "    cam_img = np.uint8(cam_img)\n",
    "    return cam_img\n",
    "\n",
    "def get_feature_map(model, image, layer_name, model_type):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    \n",
    "    def hook(module, input, output):\n",
    "        features.append(output.detach())\n",
    "    \n",
    "    if model_type == \"vit\":\n",
    "        handle = model.blocks[-1].register_forward_hook(hook)\n",
    "    elif model_type == \"resnet\":\n",
    "        handle = model.layer4.register_forward_hook(hook)\n",
    "    elif model_type == \"mobilevit\":\n",
    "        handle = model.stages[-1].register_forward_hook(hook)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model(image.unsqueeze(0))\n",
    "    handle.remove()\n",
    "    \n",
    "    feature_map = features[0][0].cpu().numpy()\n",
    "    if model_type == \"vit\":\n",
    "        if feature_map.shape[0] == 197:\n",
    "            feature_map = feature_map[1:]\n",
    "        feature_map = feature_map.mean(axis=1)\n",
    "        if feature_map.shape[0] == 196:\n",
    "            feature_map = feature_map.reshape(14, 14)\n",
    "    elif model_type in [\"resnet\", \"mobilevit\"]:\n",
    "        feature_map = feature_map.mean(axis=0)\n",
    "    \n",
    "    return feature_map\n",
    "\n",
    "def compute_accuracy(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = outputs.max(1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    return accuracy_score(all_labels, all_preds)\n",
    "\n",
    "def interactive_gradcam_viewer():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    \n",
    "    print(\"Loading ViT model...\")\n",
    "    vit_model, vit_transform, vit_type = load_model_bundle(vit_bundle_path)\n",
    "    vit_model = vit_model.to(device)\n",
    "    \n",
    "    print(\"Loading ResNet model...\")\n",
    "    resnet_model, resnet_transform, resnet_type = load_model_bundle(resnet_bundle_path)\n",
    "    resnet_model = resnet_model.to(device)\n",
    "    \n",
    "    print(\"Loading MobileViT model...\")\n",
    "    mobilevit_model, mobilevit_transform, mobilevit_type = load_model_bundle(mobilevit_bundle_path)\n",
    "    mobilevit_model = mobilevit_model.to(device)\n",
    "    \n",
    "    \n",
    "    vit_gradcam = ViTGradCAM(vit_model)\n",
    "    resnet_gradcam = ResNetGradCAM(resnet_model)\n",
    "    mobilevit_gradcam = MobileViTGradCAM(mobilevit_model)\n",
    "    \n",
    "    \n",
    "    vit_dataset = datasets.ImageFolder(data_dir, transform=vit_transform)\n",
    "    resnet_dataset = datasets.ImageFolder(data_dir, transform=resnet_transform)\n",
    "    mobilevit_dataset = datasets.ImageFolder(data_dir, transform=mobilevit_transform)\n",
    "    class_names = vit_dataset.classes\n",
    "    \n",
    "    \n",
    "    batch_size = 32\n",
    "    vit_loader = DataLoader(vit_dataset, batch_size=batch_size, shuffle=False)\n",
    "    resnet_loader = DataLoader(resnet_dataset, batch_size=batch_size, shuffle=False)\n",
    "    mobilevit_loader = DataLoader(mobilevit_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    \n",
    "    vit_acc = compute_accuracy(vit_model, vit_loader, device)\n",
    "    resnet_acc = compute_accuracy(resnet_model, resnet_loader, device)\n",
    "    mobilevit_acc = compute_accuracy(mobilevit_model, mobilevit_loader, device)\n",
    "    print(f\"ViT Test Accuracy: {vit_acc:.4f}\")\n",
    "    print(f\"ResNet Test Accuracy: {resnet_acc:.4f}\")\n",
    "    print(f\"MobileViT Test Accuracy: {mobilevit_acc:.4f}\")\n",
    "    \n",
    "    \n",
    "    img_loader = DataLoader(vit_dataset, batch_size=64, shuffle=True)\n",
    "    images, labels = next(iter(img_loader))\n",
    "    \n",
    "    \n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    \n",
    "    def update_view(idx=0):\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        img = images[idx].to(device)\n",
    "        label = labels[idx].item()\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            vit_output = vit_model(img.unsqueeze(0))\n",
    "            vit_pred = torch.argmax(vit_output, dim=1).item()\n",
    "            resnet_output = resnet_model(img.unsqueeze(0))\n",
    "            resnet_pred = torch.argmax(resnet_output, dim=1).item()\n",
    "            mobilevit_output = mobilevit_model(img.unsqueeze(0))\n",
    "            mobilevit_pred = torch.argmax(mobilevit_output, dim=1).item()\n",
    "        \n",
    "        \n",
    "        vit_cam, _ = vit_gradcam.generate_cam(img.unsqueeze(0), target_class=None)\n",
    "        resnet_cam, _ = resnet_gradcam.generate_cam(img.unsqueeze(0), target_class=None)\n",
    "        mobilevit_cam, _ = mobilevit_gradcam.generate_cam(img.unsqueeze(0), target_class=None)\n",
    "        \n",
    "        \n",
    "        vit_feature = get_feature_map(vit_model, img, \"blocks\", model_type=\"vit\")\n",
    "        resnet_feature = get_feature_map(resnet_model, img, \"layer4\", model_type=\"resnet\")\n",
    "        mobilevit_feature = get_feature_map(mobilevit_model, img, \"stages\", model_type=\"mobilevit\")\n",
    "        \n",
    "        \n",
    "        img_np = img.cpu().numpy().transpose(1, 2, 0) * std + mean\n",
    "        img_np = np.clip(img_np, 0, 1)\n",
    "        \n",
    "        \n",
    "        vit_overlay = show_cam_on_image(img_np, vit_cam)\n",
    "        resnet_overlay = show_cam_on_image(img_np, resnet_cam)\n",
    "        mobilevit_overlay = show_cam_on_image(img_np, mobilevit_cam)\n",
    "        \n",
    "        \n",
    "        fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "        \n",
    "        \n",
    "        axes[0, 0].imshow(img_np)\n",
    "        axes[0, 0].set_title(f\"Original - Class: {class_names[label]}\")\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        \n",
    "        axes[0, 1].imshow(vit_overlay)\n",
    "        axes[0, 1].set_title(f\"ViT GradCAM - Pred: {class_names[vit_pred]}\")\n",
    "        axes[0, 1].axis('off')\n",
    "        \n",
    "        \n",
    "        axes[0, 2].imshow(resnet_overlay)\n",
    "        axes[0, 2].set_title(f\"ResNet GradCAM - Pred: {class_names[resnet_pred]}\")\n",
    "        axes[0, 2].axis('off')\n",
    "        \n",
    "        \n",
    "        axes[0, 3].imshow(mobilevit_overlay)\n",
    "        axes[0, 3].set_title(f\"MobileViT GradCAM - Pred: {class_names[mobilevit_pred]}\")\n",
    "        axes[0, 3].axis('off')\n",
    "        \n",
    "        \n",
    "        axes[1, 0].imshow(img_np)\n",
    "        axes[1, 0].set_title(\"Original Image\")\n",
    "        axes[1, 0].axis('off')\n",
    "        \n",
    "        axes[1, 1].imshow(vit_feature, cmap='viridis')\n",
    "        axes[1, 1].set_title(\"ViT Feature Map\")\n",
    "        axes[1, 1].axis('off')\n",
    "        \n",
    "        axes[1, 2].imshow(resnet_feature, cmap='viridis')\n",
    "        axes[1, 2].set_title(\"ResNet Feature Map\")\n",
    "        axes[1, 2].axis('off')\n",
    "        \n",
    "        axes[1, 3].imshow(mobilevit_feature, cmap='viridis')\n",
    "        axes[1, 3].set_title(\"MobileViT Feature Map\")\n",
    "        axes[1, 3].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        print(f\"True Class: {class_names[label]}\")\n",
    "        print(f\"ViT Prediction: {class_names[vit_pred]} {'' if vit_pred == label else ''}\")\n",
    "        print(f\"ResNet Prediction: {class_names[resnet_pred]} {'' if resnet_pred == label else ''}\")\n",
    "        print(f\"MobileViT Prediction: {class_names[mobilevit_pred]} {'' if mobilevit_pred == label else ''}\")\n",
    "    \n",
    "    image_slider = widgets.IntSlider(min=0, max=len(images)-1, step=1, value=0, description='Image:')\n",
    "    widgets.interact(update_view, idx=image_slider)\n",
    "    \n",
    "    vit_gradcam.remove_hooks()\n",
    "    resnet_gradcam.remove_hooks()\n",
    "    mobilevit_gradcam.remove_hooks()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    interactive_gradcam_viewer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
