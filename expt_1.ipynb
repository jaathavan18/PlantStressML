{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eff26d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4593415a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install numpy==1.24.3 scipy==1.10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaaf68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571a2297",
   "metadata": {},
   "source": [
    "# Code 1: Vit_tiny model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb2f5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(48)\n",
    "torch.cuda.manual_seed(48)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "exp_name = \"expt_1\"\n",
    "os.makedirs(exp_name, exist_ok=True)\n",
    "\n",
    "data_dir = \"NSD\"\n",
    "train_dataset = datasets.ImageFolder(os.path.join(data_dir, \"train\"), transform=transform)\n",
    "val_dataset = datasets.ImageFolder(os.path.join(data_dir, \"val\"), transform=transform)\n",
    "test_dataset = datasets.ImageFolder(os.path.join(data_dir, \"test\"), transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "class ViTMAEModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(\"vit_tiny_patch16_224\", pretrained=True, num_classes=9)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        self.log(\"train_loss\", loss, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log(\"val_loss\", loss, on_epoch=True)\n",
    "        self.log(\"val_acc\", acc, on_epoch=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log(\"test_loss\", loss, on_epoch=True)\n",
    "        self.log(\"test_acc\", acc, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.0003)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=exp_name,\n",
    "    filename=\"best_vit_tiny_patch16_224\",\n",
    "    monitor=\"val_acc\",\n",
    "    mode=\"max\",\n",
    "    save_top_k=1\n",
    ")\n",
    "logger = CSVLogger(save_dir=exp_name, name=\"logs\")\n",
    "\n",
    "model = ViTMAEModel()\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20,\n",
    "    accelerator=\"auto\",\n",
    "    callbacks=[checkpoint_callback],\n",
    "    logger=logger,\n",
    "    default_root_dir=exp_name,\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.test(model, test_loader)\n",
    "\n",
    "best_checkpoint = f\"{exp_name}/best_vit_tiny_patch16_224.ckpt\"\n",
    "if os.path.exists(best_checkpoint):\n",
    "    vit_model = ViTMAEModel.load_from_checkpoint(best_checkpoint).model\n",
    "    vit_bundle = {\n",
    "        \"model_name\": \"vit_tiny_patch16_224\",\n",
    "        \"state_dict\": vit_model.state_dict(),\n",
    "        \"transform\": transform,\n",
    "        \"num_classes\": 9\n",
    "    }\n",
    "    torch.save(vit_bundle, f\"{exp_name}/vit_tiny_model_bundle.pth\")\n",
    "    print(f\"Saved bundle to {exp_name}/vit_tiny_model_bundle.pth\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Checkpoint not found at {best_checkpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a9c0c4",
   "metadata": {},
   "source": [
    "# Code 2: ResNet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca219110",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"expt_1\"\n",
    "os.makedirs(exp_name, exist_ok=True)\n",
    "\n",
    "class ResNetModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = models.resnet50(pretrained=True)\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, 9)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        self.log(\"test_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.0003)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=exp_name,\n",
    "    filename=\"best_resnet50\",\n",
    "    monitor=\"val_acc\",\n",
    "    mode=\"max\",\n",
    "    save_top_k=1\n",
    ")\n",
    "\n",
    "model = ResNetModel()\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20,\n",
    "    accelerator=\"auto\",\n",
    "    callbacks=[checkpoint_callback],\n",
    "    default_root_dir=exp_name,\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.test(model, test_loader)\n",
    "\n",
    "best_checkpoint = f\"{exp_name}/best_resnet50.ckpt\"\n",
    "if os.path.exists(best_checkpoint):\n",
    "    resnet_model = ResNetModel.load_from_checkpoint(best_checkpoint).model\n",
    "    resnet_bundle = {\n",
    "        \"model_name\": \"resnet50\",\n",
    "        \"state_dict\": resnet_model.state_dict(),\n",
    "        \"transform\": transform,\n",
    "        \"num_classes\": 9\n",
    "    }\n",
    "    torch.save(resnet_bundle, f\"{exp_name}/resnet50_model_bundle.pth\")\n",
    "    print(f\"Saved bundle to {exp_name}/resnet50_model_bundle.pth\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Checkpoint not found at {best_checkpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192f1e91",
   "metadata": {},
   "source": [
    "# Code 3: MobileVit XS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb24d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(48)\n",
    "torch.cuda.manual_seed(48)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "exp_name = \"expt_1\"\n",
    "os.makedirs(exp_name, exist_ok=True)\n",
    "\n",
    "data_dir = \"NSD\"\n",
    "train_dataset = datasets.ImageFolder(os.path.join(data_dir, \"train\"), transform=transform)\n",
    "val_dataset = datasets.ImageFolder(os.path.join(data_dir, \"val\"), transform=transform)\n",
    "test_dataset = datasets.ImageFolder(os.path.join(data_dir, \"test\"), transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "class MobileViTModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(\"mobilevit_xs\", pretrained=True, num_classes=9)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        self.log(\"train_loss\", loss, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log(\"val_loss\", loss, on_epoch=True)\n",
    "        self.log(\"val_acc\", acc, on_epoch=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log(\"test_loss\", loss, on_epoch=True)\n",
    "        self.log(\"test_acc\", acc, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.0003)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=exp_name,\n",
    "    filename=\"best_mobilevit_xs\",\n",
    "    monitor=\"val_acc\",\n",
    "    mode=\"max\",\n",
    "    save_top_k=1\n",
    ")\n",
    "logger = CSVLogger(save_dir=exp_name, name=\"logs\")\n",
    "\n",
    "model = MobileViTModel()\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20,\n",
    "    accelerator=\"auto\",\n",
    "    callbacks=[checkpoint_callback],\n",
    "    logger=logger,\n",
    "    default_root_dir=exp_name,\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.test(model, test_loader)\n",
    "\n",
    "best_checkpoint = f\"{exp_name}/best_mobilevit_xs.ckpt\"\n",
    "if os.path.exists(best_checkpoint):\n",
    "    mobilevit_model = MobileViTModel.load_from_checkpoint(best_checkpoint).model\n",
    "    mobilevit_bundle = {\n",
    "        \"model_name\": \"mobilevit_xs\",\n",
    "        \"state_dict\": mobilevit_model.state_dict(),\n",
    "        \"transform\": transform,\n",
    "        \"num_classes\": 9\n",
    "    }\n",
    "    torch.save(mobilevit_bundle, f\"{exp_name}/mobilevit_xs_model_bundle.pth\")\n",
    "    print(f\"Saved bundle to {exp_name}/mobilevit_xs_model_bundle.pth\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Checkpoint not found at {best_checkpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0f1c52",
   "metadata": {},
   "source": [
    "# Code 3.5: Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73d3d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(48)\n",
    "np.random.seed(48)\n",
    "\n",
    "def load_model_bundle(bundle_path, model_name):\n",
    "    bundle = torch.load(bundle_path, map_location=\"cpu\", weights_only=False)\n",
    "    if bundle[\"model_name\"] != model_name:\n",
    "        raise ValueError(f\"Expected {model_name}, got {bundle['model_name']}\")\n",
    "    if model_name == \"resnet50\":\n",
    "        model = models.resnet50(weights=None)\n",
    "        model.fc = nn.Linear(model.fc.in_features, bundle[\"num_classes\"])\n",
    "    else:\n",
    "        model = timm.create_model(model_name, pretrained=False, num_classes=bundle[\"num_classes\"])\n",
    "    model.load_state_dict(bundle[\"state_dict\"])\n",
    "    model.eval()\n",
    "    return model, bundle[\"transform\"]\n",
    "\n",
    "def evaluate(model, loader, device, desc=\"\"):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for images, lbls in loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "            labels.extend(lbls.numpy())\n",
    "    inference_time = time.time() - start_time\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(labels, preds) * 100,\n",
    "        \"precision\": precision_score(labels, preds, average=\"macro\"),\n",
    "        \"recall\": recall_score(labels, preds, average=\"macro\"),\n",
    "        \"f1\": f1_score(labels, preds, average=\"macro\"),\n",
    "        \"inference_time\": inference_time\n",
    "    }\n",
    "    return preds, labels, metrics\n",
    "\n",
    "exp_name = \"expt_1\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_dir = \"NSD\"\n",
    "test_dataset = datasets.ImageFolder(os.path.join(data_dir, \"test\"), transform=None)\n",
    "\n",
    "models_config = [\n",
    "    {\"name\": \"ViT-Tiny\", \"model_name\": \"vit_tiny_patch16_224\", \"bundle_path\": f\"{exp_name}/vit_tiny_model_bundle.pth\"},\n",
    "    {\"name\": \"ResNet-50\", \"model_name\": \"resnet50\", \"bundle_path\": f\"{exp_name}/resnet50_model_bundle.pth\"},\n",
    "    {\"name\": \"MobileViT-XS\", \"model_name\": \"mobilevit_xs\", \"bundle_path\": f\"{exp_name}/mobilevit_xs_model_bundle.pth\"}\n",
    "]\n",
    "\n",
    "all_metrics = {}\n",
    "mean, std = np.array([0.485, 0.456, 0.406]), np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "for config in models_config:\n",
    "    name, model_name, bundle_path = config[\"name\"], config[\"model_name\"], config[\"bundle_path\"]\n",
    "    print(f\"\\nTesting {name}...\")\n",
    "    \n",
    "    model, transform = load_model_bundle(bundle_path, model_name)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    test_dataset.transform = transform\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "    \n",
    "    _, _, metrics = evaluate(model, test_loader, device, \"Balanced Test\")\n",
    "    \n",
    "    all_metrics[name] = metrics\n",
    "    \n",
    "    print(f\"{name} Results:\")\n",
    "    for k, v in metrics.items():\n",
    "        unit = \"s\" if \"time\" in k else \"\"\n",
    "        print(f\"  {k.capitalize()}: {v:.2f}{unit}\")\n",
    "    \n",
    "    os.makedirs(exp_name, exist_ok=True)\n",
    "    json_path = f\"{exp_name}/{model_name}_test_results.json\"\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    print(f\"Saved metrics to {json_path}\")\n",
    "    \n",
    "    display_images, display_preds, display_labels = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "            images = images.to(device)\n",
    "            preds = model(images).argmax(dim=1).cpu().numpy()\n",
    "            if i == 0:\n",
    "                display_images = images[:5].cpu()\n",
    "                display_preds = preds[:5]\n",
    "                display_labels = labels[:5].numpy()\n",
    "            break\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 5, figsize=(15, 3))\n",
    "    for i, (img, pred, true) in enumerate(zip(display_images, display_preds, display_labels)):\n",
    "        img = img.permute(1, 2, 0).numpy() * std + mean\n",
    "        img = np.clip(img, 0, 1)\n",
    "        axs[i].imshow(img)\n",
    "        axs[i].set_title(f\"Pred: {pred}\\nTrue: {true}\")\n",
    "        axs[i].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    img_path = f\"{exp_name}/{model_name}_prediction_examples.png\"\n",
    "    plt.savefig(img_path)\n",
    "    print(f\"Saved predictions to {img_path}\")\n",
    "\n",
    "combined_json_path = f\"{exp_name}/combined_test_results.json\"\n",
    "with open(combined_json_path, \"w\") as f:\n",
    "    json.dump(all_metrics, f, indent=4)\n",
    "print(f\"\\nSaved combined metrics to {combined_json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdf3ff4",
   "metadata": {},
   "source": [
    "# Code 4: Feature Visualization Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6caed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_bundle_path = \"expt_1/vit_tiny_model_bundle.pth\"\n",
    "resnet_bundle_path = \"expt_1/resnet50_model_bundle.pth\"\n",
    "mobilevit_bundle_path = \"expt_1/mobilevit_xs_model_bundle.pth\"\n",
    "data_dir = \"NSD/test\"\n",
    "output_dir = \"expt_1/feature_visualization\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def set_seed(seed=48):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()\n",
    "\n",
    "def get_feature_extractor(model, model_type):\n",
    "    \"\"\"\n",
    "    Returns a feature extractor function for the given model type\n",
    "    \"\"\"\n",
    "    if model_type == \"vit\":\n",
    "        def extract_features(x):\n",
    "            x = model.patch_embed(x)\n",
    "            cls_token = model.cls_token.expand(x.shape[0], -1, -1)\n",
    "            x = torch.cat((cls_token, x), dim=1)\n",
    "            if hasattr(model, 'pos_drop'):\n",
    "                x = model.pos_drop(x + model.pos_embed)\n",
    "            else:\n",
    "                x = x + model.pos_embed\n",
    "                \n",
    "            \n",
    "            for blk in model.blocks:\n",
    "                x = blk(x)\n",
    "                \n",
    "            x = model.norm(x)\n",
    "            \n",
    "            return x[:, 0]\n",
    "            \n",
    "    elif model_type == \"mobilevit\":\n",
    "        \n",
    "        def extract_features(x):\n",
    "            \n",
    "            x = model.stem(x)\n",
    "            for stage in model.stages:\n",
    "                x = stage(x)\n",
    "            \n",
    "            x = nn.functional.adaptive_avg_pool2d(x, (1, 1))\n",
    "            x = x.flatten(1)\n",
    "            return x\n",
    "            \n",
    "    elif model_type == \"resnet\":\n",
    "        \n",
    "        def extract_features(x):\n",
    "            x = model.conv1(x)\n",
    "            x = model.bn1(x)\n",
    "            x = model.relu(x)\n",
    "            x = model.maxpool(x)\n",
    "            \n",
    "            x = model.layer1(x)\n",
    "            x = model.layer2(x)\n",
    "            x = model.layer3(x)\n",
    "            x = model.layer4(x)\n",
    "            \n",
    "            x = model.avgpool(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            return x\n",
    "    \n",
    "    return extract_features\n",
    "\n",
    "def load_model_bundle(bundle_path):\n",
    "    \"\"\"\n",
    "    Load a model bundle and return the model, transform, and model type\n",
    "    \"\"\"\n",
    "    bundle = torch.load(bundle_path, map_location=\"cpu\", weights_only=False)\n",
    "    model_name = bundle[\"model_name\"]\n",
    "    \n",
    "    if \"vit\" in model_name or \"mobilevit\" in model_name:\n",
    "        model = timm.create_model(model_name, pretrained=False, num_classes=bundle[\"num_classes\"])\n",
    "        model_type = \"vit\" if \"vit\" in model_name and \"mobilevit\" not in model_name else \"mobilevit\"\n",
    "    elif \"resnet\" in model_name:\n",
    "        model = models.resnet50(pretrained=False)\n",
    "        model.fc = nn.Linear(model.fc.in_features, bundle[\"num_classes\"])\n",
    "        model_type = \"resnet\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_name}\")\n",
    "    \n",
    "    model.load_state_dict(bundle[\"state_dict\"])\n",
    "    model.eval()\n",
    "    \n",
    "    return model, bundle[\"transform\"], model_type\n",
    "\n",
    "def extract_features(model, feature_extractor, data_loader, device):\n",
    "    \"\"\"\n",
    "    Extract features from a model using the provided feature extractor\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images = images.to(device)\n",
    "            batch_features = feature_extractor(images)\n",
    "            features.append(batch_features.cpu().numpy())\n",
    "            labels.append(targets.numpy())\n",
    "    \n",
    "    features = np.concatenate(features, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "def calculate_separation_metrics(features, labels):\n",
    "    \"\"\"\n",
    "    Calculate separation metrics for feature space:\n",
    "    - Average intra-class distance\n",
    "    - Average inter-class distance\n",
    "    - Separation ratio (inter/intra)\n",
    "    \"\"\"\n",
    "    classes = np.unique(labels)\n",
    "    \n",
    "    \n",
    "    centers = []\n",
    "    for c in classes:\n",
    "        centers.append(np.mean(features[labels == c], axis=0))\n",
    "    centers = np.array(centers)\n",
    "    \n",
    "    \n",
    "    intra_class_distances = []\n",
    "    for c in classes:\n",
    "        class_features = features[labels == c]\n",
    "        center = centers[int(c)]\n",
    "        distances = np.sqrt(np.sum((class_features - center)**2, axis=1))\n",
    "        intra_class_distances.append(np.mean(distances))\n",
    "    \n",
    "    avg_intra_class_distance = np.mean(intra_class_distances)\n",
    "    \n",
    "    \n",
    "    inter_class_distances = []\n",
    "    for i in range(len(centers)):\n",
    "        for j in range(i+1, len(centers)):\n",
    "            distance = np.sqrt(np.sum((centers[i] - centers[j])**2))\n",
    "            inter_class_distances.append(distance)\n",
    "    \n",
    "    avg_inter_class_distance = np.mean(inter_class_distances)\n",
    "    \n",
    "    \n",
    "    separation_ratio = avg_inter_class_distance / avg_intra_class_distance\n",
    "    \n",
    "    return {\n",
    "        'avg_intra_class_distance': avg_intra_class_distance,\n",
    "        'avg_inter_class_distance': avg_inter_class_distance,\n",
    "        'separation_ratio': separation_ratio\n",
    "    }\n",
    "\n",
    "def plot_class_distribution(labels, class_names, title, filename=None):\n",
    "    \"\"\"\n",
    "    Plot class distribution\n",
    "    \"\"\"\n",
    "    class_counts = np.bincount(labels, minlength=len(class_names))\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(len(class_names)), class_counts)\n",
    "    plt.xticks(range(len(class_names)), class_names, rotation=45)\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(title)\n",
    "    \n",
    "    if filename:\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(\"Loading ViT model...\")\n",
    "vit_model, vit_transform, vit_model_type = load_model_bundle(vit_bundle_path)\n",
    "vit_model = vit_model.to(device)\n",
    "vit_feature_extractor = get_feature_extractor(vit_model, vit_model_type)\n",
    "\n",
    "print(\"Loading ResNet model...\")\n",
    "resnet_model, resnet_transform, resnet_model_type = load_model_bundle(resnet_bundle_path)\n",
    "resnet_model = resnet_model.to(device)\n",
    "resnet_feature_extractor = get_feature_extractor(resnet_model, resnet_model_type)\n",
    "\n",
    "print(\"Loading MobileViT model...\")\n",
    "mobilevit_model, mobilevit_transform, mobilevit_model_type = load_model_bundle(mobilevit_bundle_path)\n",
    "mobilevit_model = mobilevit_model.to(device)\n",
    "mobilevit_feature_extractor = get_feature_extractor(mobilevit_model, mobilevit_model_type)\n",
    "\n",
    "print(\"Loading full dataset...\")\n",
    "full_dataset = datasets.ImageFolder(data_dir, transform=vit_transform)\n",
    "class_names = full_dataset.classes\n",
    "class_counts = np.bincount([label for _, label in full_dataset.samples])\n",
    "print(\"Class distribution:\", dict(zip(class_names, class_counts)))\n",
    "\n",
    "print(\"Creating imbalanced dataset...\")\n",
    "min_class = np.argmin(class_counts)\n",
    "print(f\"Minority class: {class_names[min_class]} with {class_counts[min_class]} samples\")\n",
    "min_indices = [i for i, (_, lbl) in enumerate(full_dataset.samples) if lbl == min_class]\n",
    "min_keep_indices = np.random.choice(min_indices, size=len(min_indices) // 2, replace=False)\n",
    "other_indices = [i for i, (_, lbl) in enumerate(full_dataset.samples) if lbl != min_class]\n",
    "imbalanced_indices = other_indices + list(min_keep_indices)\n",
    "imbalanced_dataset = Subset(full_dataset, imbalanced_indices)\n",
    "\n",
    "batch_size = 32\n",
    "full_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "imbalanced_loader = DataLoader(imbalanced_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "print(\"Extracting features from full dataset...\")\n",
    "vit_features_full, vit_labels_full = extract_features(vit_model, vit_feature_extractor, full_loader, device)\n",
    "resnet_features_full, resnet_labels_full = extract_features(resnet_model, resnet_feature_extractor, full_loader, device)\n",
    "mobilevit_features_full, mobilevit_labels_full = extract_features(mobilevit_model, mobilevit_feature_extractor, full_loader, device)\n",
    "\n",
    "print(\"Extracting features from imbalanced dataset...\")\n",
    "vit_features_imbalanced, vit_labels_imbalanced = extract_features(vit_model, vit_feature_extractor, imbalanced_loader, device)\n",
    "resnet_features_imbalanced, resnet_labels_imbalanced = extract_features(resnet_model, resnet_feature_extractor, imbalanced_loader, device)\n",
    "mobilevit_features_imbalanced, mobilevit_labels_imbalanced = extract_features(mobilevit_model, mobilevit_feature_extractor, imbalanced_loader, device)\n",
    "\n",
    "plot_class_distribution(vit_labels_full, class_names, \"Full Dataset Class Distribution\", \n",
    "                        os.path.join(output_dir, \"full_class_distribution.png\"))\n",
    "plot_class_distribution(vit_labels_imbalanced, class_names, \"Imbalanced Dataset Class Distribution\", \n",
    "                        os.path.join(output_dir, \"imbalanced_class_distribution.png\"))\n",
    "\n",
    "# Apply PCA to full and imbalanced datasets\n",
    "print(\"Applying PCA to full dataset features...\")\n",
    "vit_pca_full = PCA(n_components=2)\n",
    "vit_pca_full_result = vit_pca_full.fit_transform(vit_features_full)\n",
    "resnet_pca_full = PCA(n_components=2)\n",
    "resnet_pca_full_result = resnet_pca_full.fit_transform(resnet_features_full)\n",
    "mobilevit_pca_full = PCA(n_components=2)\n",
    "mobilevit_pca_full_result = mobilevit_pca_full.fit_transform(mobilevit_features_full)\n",
    "\n",
    "print(\"Applying PCA to imbalanced dataset features...\")\n",
    "vit_pca_imbalanced = PCA(n_components=2)\n",
    "vit_pca_imbalanced_result = vit_pca_imbalanced.fit_transform(vit_features_imbalanced)\n",
    "resnet_pca_imbalanced = PCA(n_components=2)\n",
    "resnet_pca_imbalanced_result = resnet_pca_imbalanced.fit_transform(resnet_features_imbalanced)\n",
    "mobilevit_pca_imbalanced = PCA(n_components=2)\n",
    "mobilevit_pca_imbalanced_result = mobilevit_pca_imbalanced.fit_transform(mobilevit_features_imbalanced)\n",
    "\n",
    "# Apply t-SNE to full and imbalanced datasets\n",
    "print(\"Applying t-SNE to full dataset features (this may take a while)...\")\n",
    "vit_tsne_full = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "vit_tsne_full_result = vit_tsne_full.fit_transform(vit_features_full)\n",
    "resnet_tsne_full = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "resnet_tsne_full_result = resnet_tsne_full.fit_transform(resnet_features_full)\n",
    "mobilevit_tsne_full = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "mobilevit_tsne_full_result = mobilevit_tsne_full.fit_transform(mobilevit_features_full)\n",
    "\n",
    "print(\"Applying t-SNE to imbalanced dataset features (this may take a while)...\")\n",
    "vit_tsne_imbalanced = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "vit_tsne_imbalanced_result = vit_tsne_imbalanced.fit_transform(vit_features_imbalanced)\n",
    "resnet_tsne_imbalanced = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "resnet_tsne_imbalanced_result = resnet_tsne_imbalanced.fit_transform(resnet_features_imbalanced)\n",
    "mobilevit_tsne_imbalanced = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "mobilevit_tsne_imbalanced_result = mobilevit_tsne_imbalanced.fit_transform(mobilevit_features_imbalanced)\n",
    "\n",
    "# Plot PCA results for full vs imbalanced\n",
    "plt.figure(figsize=(30, 10))\n",
    "\n",
    "# ViT PCA - Full Dataset\n",
    "plt.subplot(2, 3, 1)\n",
    "scatter = plt.scatter(vit_pca_full_result[:, 0], vit_pca_full_result[:, 1], c=vit_labels_full, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(scatter, boundaries=np.arange(len(class_names)+1)-0.5).set_ticks(np.arange(len(class_names)))\n",
    "plt.title('PCA: ViT features - Full Dataset', fontsize=14)\n",
    "plt.xlabel('Principal Component 1', fontsize=12)\n",
    "plt.ylabel('Principal Component 2', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# ViT PCA - Imbalanced Dataset\n",
    "plt.subplot(2, 3, 4)\n",
    "scatter = plt.scatter(vit_pca_imbalanced_result[:, 0], vit_pca_imbalanced_result[:, 1], c=vit_labels_imbalanced, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(scatter, boundaries=np.arange(len(class_names)+1)-0.5).set_ticks(np.arange(len(class_names)))\n",
    "plt.title('PCA: ViT features - Imbalanced Dataset', fontsize=14)\n",
    "plt.xlabel('Principal Component 1', fontsize=12)\n",
    "plt.ylabel('Principal Component 2', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# ResNet PCA - Full Dataset\n",
    "plt.subplot(2, 3, 2)\n",
    "scatter = plt.scatter(resnet_pca_full_result[:, 0], resnet_pca_full_result[:, 1], c=resnet_labels_full, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(scatter, boundaries=np.arange(len(class_names)+1)-0.5).set_ticks(np.arange(len(class_names)))\n",
    "plt.title('PCA: ResNet features - Full Dataset', fontsize=14)\n",
    "plt.xlabel('Principal Component 1', fontsize=12)\n",
    "plt.ylabel('Principal Component 2', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# ResNet PCA - Imbalanced Dataset\n",
    "plt.subplot(2, 3, 5)\n",
    "scatter = plt.scatter(resnet_pca_imbalanced_result[:, 0], resnet_pca_imbalanced_result[:, 1], c=resnet_labels_imbalanced, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(scatter, boundaries=np.arange(len(class_names)+1)-0.5).set_ticks(np.arange(len(class_names)))\n",
    "plt.title('PCA: ResNet features - Imbalanced Dataset', fontsize=14)\n",
    "plt.xlabel('Principal Component 1', fontsize=12)\n",
    "plt.ylabel('Principal Component 2', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# MobileViT PCA - Full Dataset\n",
    "plt.subplot(2, 3, 3)\n",
    "scatter = plt.scatter(mobilevit_pca_full_result[:, 0], mobilevit_pca_full_result[:, 1], c=mobilevit_labels_full, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(scatter, boundaries=np.arange(len(class_names)+1)-0.5).set_ticks(np.arange(len(class_names)))\n",
    "plt.title('PCA: MobileViT features - Full Dataset', fontsize=14)\n",
    "plt.xlabel('Principal Component 1', fontsize=12)\n",
    "plt.ylabel('Principal Component 2', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# MobileViT PCA - Imbalanced Dataset\n",
    "plt.subplot(2, 3, 6)\n",
    "scatter = plt.scatter(mobilevit_pca_imbalanced_result[:, 0], mobilevit_pca_imbalanced_result[:, 1], c=mobilevit_labels_imbalanced, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(scatter, boundaries=np.arange(len(class_names)+1)-0.5).set_ticks(np.arange(len(class_names)))\n",
    "plt.title('PCA: MobileViT features - Imbalanced Dataset', fontsize=14)\n",
    "plt.xlabel('Principal Component 1', fontsize=12)\n",
    "plt.ylabel('Principal Component 2', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'pca_comparison_imbalanced.png'))\n",
    "plt.close()\n",
    "\n",
    "# Plot t-SNE results for full vs imbalanced\n",
    "plt.figure(figsize=(30, 10))\n",
    "\n",
    "# ViT t-SNE - Full Dataset\n",
    "plt.subplot(2, 3, 1)\n",
    "scatter = plt.scatter(vit_tsne_full_result[:, 0], vit_tsne_full_result[:, 1], c=vit_labels_full, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(scatter, boundaries=np.arange(len(class_names)+1)-0.5).set_ticks(np.arange(len(class_names)))\n",
    "plt.title('t-SNE: ViT features - Full Dataset', fontsize=14)\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# ViT t-SNE - Imbalanced Dataset\n",
    "plt.subplot(2, 3, 4)\n",
    "scatter = plt.scatter(vit_tsne_imbalanced_result[:, 0], vit_tsne_imbalanced_result[:, 1], c=vit_labels_imbalanced, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(scatter, boundaries=np.arange(len(class_names)+1)-0.5).set_ticks(np.arange(len(class_names)))\n",
    "plt.title('t-SNE: ViT features - Imbalanced Dataset', fontsize=14)\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# ResNet t-SNE - Full Dataset\n",
    "plt.subplot(2, 3, 2)\n",
    "scatter = plt.scatter(resnet_tsne_full_result[:, 0], resnet_tsne_full_result[:, 1], c=resnet_labels_full, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(scatter, boundaries=np.arange(len(class_names)+1)-0.5).set_ticks(np.arange(len(class_names)))\n",
    "plt.title('t-SNE: ResNet features - Full Dataset', fontsize=14)\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# ResNet t-SNE - Imbalanced Dataset\n",
    "plt.subplot(2, 3, 5)\n",
    "scatter = plt.scatter(resnet_tsne_imbalanced_result[:, 0], resnet_tsne_imbalanced_result[:, 1], c=resnet_labels_imbalanced, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(scatter, boundaries=np.arange(len(class_names)+1)-0.5).set_ticks(np.arange(len(class_names)))\n",
    "plt.title('t-SNE: ResNet features - Imbalanced Dataset', fontsize=14)\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# MobileViT t-SNE - Full Dataset\n",
    "plt.subplot(2, 3, 3)\n",
    "scatter = plt.scatter(mobilevit_tsne_full_result[:, 0], mobilevit_tsne_full_result[:, 1], c=mobilevit_labels_full, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(scatter, boundaries=np.arange(len(class_names)+1)-0.5).set_ticks(np.arange(len(class_names)))\n",
    "plt.title('t-SNE: MobileViT features - Full Dataset', fontsize=14)\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# MobileViT t-SNE - Imbalanced Dataset\n",
    "plt.subplot(2, 3, 6)\n",
    "scatter = plt.scatter(mobilevit_tsne_imbalanced_result[:, 0], mobilevit_tsne_imbalanced_result[:, 1], c=mobilevit_labels_imbalanced, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(scatter, boundaries=np.arange(len(class_names)+1)-0.5).set_ticks(np.arange(len(class_names)))\n",
    "plt.title('t-SNE: MobileViT features - Imbalanced Dataset', fontsize=14)\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'tsne_comparison_imbalanced.png'))\n",
    "plt.close()\n",
    "\n",
    "min_class_name = class_names[min_class]\n",
    "\n",
    "def add_minority_flag(labels, min_class):\n",
    "    return ['Minority Class' if l == min_class else 'Other Classes' for l in labels]\n",
    "\n",
    "vit_df_full = pd.DataFrame({\n",
    "    'x': vit_tsne_full_result[:, 0],\n",
    "    'y': vit_tsne_full_result[:, 1],\n",
    "    'class': [class_names[l] for l in vit_labels_full],\n",
    "    'minority_flag': add_minority_flag(vit_labels_full, min_class)\n",
    "})\n",
    "\n",
    "resnet_df_full = pd.DataFrame({\n",
    "    'x': resnet_tsne_full_result[:, 0],\n",
    "    'y': resnet_tsne_full_result[:, 1],\n",
    "    'class': [class_names[l] for l in resnet_labels_full],\n",
    "    'minority_flag': add_minority_flag(resnet_labels_full, min_class)\n",
    "})\n",
    "\n",
    "mobilevit_df_full = pd.DataFrame({\n",
    "    'x': mobilevit_tsne_full_result[:, 0],\n",
    "    'y': mobilevit_tsne_full_result[:, 1],\n",
    "    'class': [class_names[l] for l in mobilevit_labels_full],\n",
    "    'minority_flag': add_minority_flag(mobilevit_labels_full, min_class)\n",
    "})\n",
    "\n",
    "vit_df_imbalanced = pd.DataFrame({\n",
    "    'x': vit_tsne_imbalanced_result[:, 0],\n",
    "    'y': vit_tsne_imbalanced_result[:, 1],\n",
    "    'class': [class_names[l] for l in vit_labels_imbalanced],\n",
    "    'minority_flag': add_minority_flag(vit_labels_imbalanced, min_class)\n",
    "})\n",
    "\n",
    "resnet_df_imbalanced = pd.DataFrame({\n",
    "    'x': resnet_tsne_imbalanced_result[:, 0],\n",
    "    'y': resnet_tsne_imbalanced_result[:, 1],\n",
    "    'class': [class_names[l] for l in resnet_labels_imbalanced],\n",
    "    'minority_flag': add_minority_flag(resnet_labels_imbalanced, min_class)\n",
    "})\n",
    "\n",
    "mobilevit_df_imbalanced = pd.DataFrame({\n",
    "    'x': mobilevit_tsne_imbalanced_result[:, 0],\n",
    "    'y': mobilevit_tsne_imbalanced_result[:, 1],\n",
    "    'class': [class_names[l] for l in mobilevit_labels_imbalanced],\n",
    "    'minority_flag': add_minority_flag(mobilevit_labels_imbalanced, min_class)\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.scatterplot(\n",
    "    x='x', y='y',\n",
    "    hue='class',\n",
    "    palette=sns.color_palette(\"hls\", len(class_names)),\n",
    "    data=vit_df_full,\n",
    "    legend=\"full\",\n",
    "    alpha=0.8\n",
    ")\n",
    "plt.title('t-SNE: ViT features - Full Dataset', fontsize=18)\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=14)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'vit_tsne_full_labeled.png'))\n",
    "plt.close()\n",
    "\n",
    "# ViT - Imbalanced Dataset\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.scatterplot(\n",
    "    x='x', y='y',\n",
    "    hue='class',\n",
    "    palette=sns.color_palette(\"hls\", len(class_names)),\n",
    "    data=vit_df_imbalanced,\n",
    "    legend=\"full\",\n",
    "    alpha=0.8\n",
    ")\n",
    "plt.title('t-SNE: ViT features - Imbalanced Dataset', fontsize=18)\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=14)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'vit_tsne_imbalanced_labeled.png'))\n",
    "plt.close()\n",
    "\n",
    "# ResNet - Full Dataset\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.scatterplot(\n",
    "    x='x', y='y',\n",
    "    hue='class',\n",
    "    palette=sns.color_palette(\"hls\", len(class_names)),\n",
    "    data=resnet_df_full,\n",
    "    legend=\"full\",\n",
    "    alpha=0.8\n",
    ")\n",
    "plt.title('t-SNE: ResNet features - Full Dataset', fontsize=18)\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=14)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'resnet_tsne_full_labeled.png'))\n",
    "plt.close()\n",
    "\n",
    "# ResNet - Imbalanced Dataset\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.scatterplot(\n",
    "    x='x', y='y',\n",
    "    hue='class',\n",
    "    palette=sns.color_palette(\"hls\", len(class_names)),\n",
    "    data=resnet_df_imbalanced,\n",
    "    legend=\"full\",\n",
    "    alpha=0.8\n",
    ")\n",
    "plt.title('t-SNE: ResNet features - Imbalanced Dataset', fontsize=18)\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=14)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'resnet_tsne_imbalanced_labeled.png'))\n",
    "plt.close()\n",
    "\n",
    "# MobileViT - Full Dataset\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.scatterplot(\n",
    "    x='x', y='y',\n",
    "    hue='class',\n",
    "    palette=sns.color_palette(\"hls\", len(class_names)),\n",
    "    data=mobilevit_df_full,\n",
    "    legend=\"full\",\n",
    "    alpha=0.8\n",
    ")\n",
    "plt.title('t-SNE: MobileViT features - Full Dataset', fontsize=18)\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=14)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'mobilevit_tsne_full_labeled.png'))\n",
    "plt.close()\n",
    "\n",
    "# MobileViT - Imbalanced Dataset\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.scatterplot(\n",
    "    x='x', y='y',\n",
    "    hue='class',\n",
    "    palette=sns.color_palette(\"hls\", len(class_names)),\n",
    "    data=mobilevit_df_imbalanced,\n",
    "    legend=\"full\",\n",
    "    alpha=0.8\n",
    ")\n",
    "plt.title('t-SNE: MobileViT features - Imbalanced Dataset', fontsize=18)\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=14)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'mobilevit_tsne_imbalanced_labeled.png'))\n",
    "plt.close()\n",
    "\n",
    "# ViT - Full vs Imbalanced with highlighted minority class\n",
    "fig, axes = plt.subplots(1, 3, figsize=(30, 10))\n",
    "\n",
    "# ViT Full dataset\n",
    "sns.scatterplot(\n",
    "    x='x', y='y',\n",
    "    hue='minority_flag',\n",
    "    palette={'Minority Class': 'red', 'Other Classes': 'gray'},\n",
    "    data=vit_df_full,\n",
    "    alpha=0.8,\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title('ViT: Full Dataset\\nRed: Minority Class', fontsize=16)\n",
    "axes[0].set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "axes[0].set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "\n",
    "# ViT Imbalanced dataset\n",
    "sns.scatterplot(\n",
    "    x='x', y='y',\n",
    "    hue='minority_flag',\n",
    "    palette={'Minority Class': 'red', 'Other Classes': 'gray'},\n",
    "    data=vit_df_imbalanced,\n",
    "    alpha=0.8,\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title('ViT: Imbalanced Dataset\\nRed: Minority Class', fontsize=16)\n",
    "axes[1].set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "axes[1].set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "\n",
    "# MobileViT Full dataset\n",
    "sns.scatterplot(\n",
    "    x='x', y='y',\n",
    "    hue='minority_flag',\n",
    "    palette={'Minority Class': 'red', 'Other Classes': 'gray'},\n",
    "    data=mobilevit_df_full,\n",
    "    alpha=0.8,\n",
    "    ax=axes[2]\n",
    ")\n",
    "axes[2].set_title('MobileViT: Full Dataset\\nRed: Minority Class', fontsize=16)\n",
    "axes[2].set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "axes[2].set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'vit_mobilevit_minority_class_comparison.png'))\n",
    "plt.close()\n",
    "\n",
    "# ResNet and MobileViT - Full vs Imbalanced with highlighted minority class\n",
    "fig, axes = plt.subplots(1, 3, figsize=(30, 10))\n",
    "\n",
    "# ResNet Full dataset\n",
    "sns.scatterplot(\n",
    "    x='x', y='y',\n",
    "    hue='minority_flag',\n",
    "    palette={'Minority Class': 'red', 'Other Classes': 'gray'},\n",
    "    data=resnet_df_full,\n",
    "    alpha=0.8,\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title('ResNet: Full Dataset\\nRed: Minority Class', fontsize=16)\n",
    "axes[0].set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "axes[0].set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "\n",
    "# ResNet Imbalanced dataset\n",
    "sns.scatterplot(\n",
    "    x='x', y='y',\n",
    "    hue='minority_flag',\n",
    "    palette={'Minority Class': 'red', 'Other Classes': 'gray'},\n",
    "    data=resnet_df_imbalanced,\n",
    "    alpha=0.8,\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title('ResNet: Imbalanced Dataset\\nRed: Minority Class', fontsize=16)\n",
    "axes[1].set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "axes[1].set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "\n",
    "# MobileViT Imbalanced dataset\n",
    "sns.scatterplot(\n",
    "    x='x', y='y',\n",
    "    hue='minority_flag',\n",
    "    palette={'Minority Class': 'red', 'Other Classes': 'gray'},\n",
    "    data=mobilevit_df_imbalanced,\n",
    "    alpha=0.8,\n",
    "    ax=axes[2]\n",
    ")\n",
    "axes[2].set_title('MobileViT: Imbalanced Dataset\\nRed: Minority Class', fontsize=16)\n",
    "axes[2].set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "axes[2].set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'resnet_mobilevit_minority_class_comparison.png'))\n",
    "plt.close()\n",
    "\n",
    "\n",
    "metrics = {}\n",
    "\n",
    "# Full dataset metrics\n",
    "metrics['vit_full_pca'] = calculate_separation_metrics(vit_pca_full_result, vit_labels_full)\n",
    "metrics['resnet_full_pca'] = calculate_separation_metrics(resnet_pca_full_result, resnet_labels_full)\n",
    "metrics['mobilevit_full_pca'] = calculate_separation_metrics(mobilevit_pca_full_result, mobilevit_labels_full)\n",
    "metrics['vit_full_tsne'] = calculate_separation_metrics(vit_tsne_full_result, vit_labels_full)\n",
    "metrics['resnet_full_tsne'] = calculate_separation_metrics(resnet_tsne_full_result, resnet_labels_full)\n",
    "metrics['mobilevit_full_tsne'] = calculate_separation_metrics(mobilevit_tsne_full_result, mobilevit_labels_full)\n",
    "\n",
    "# Imbalanced dataset metrics\n",
    "metrics['vit_imbalanced_pca'] = calculate_separation_metrics(vit_pca_imbalanced_result, vit_labels_imbalanced)\n",
    "metrics['resnet_imbalanced_pca'] = calculate_separation_metrics(resnet_pca_imbalanced_result, resnet_labels_imbalanced)\n",
    "metrics['mobilevit_imbalanced_pca'] = calculate_separation_metrics(mobilevit_pca_imbalanced_result, mobilevit_labels_imbalanced)\n",
    "metrics['vit_imbalanced_tsne'] = calculate_separation_metrics(vit_tsne_imbalanced_result, vit_labels_imbalanced)\n",
    "metrics['resnet_imbalanced_tsne'] = calculate_separation_metrics(resnet_tsne_imbalanced_result, resnet_labels_imbalanced)\n",
    "metrics['mobilevit_imbalanced_tsne'] = calculate_separation_metrics(mobilevit_tsne_imbalanced_result, mobilevit_labels_imbalanced)\n",
    "\n",
    "# Metrics dataframe\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Model': ['ViT (Full, PCA)', 'ResNet (Full, PCA)', 'MobileViT (Full, PCA)', \n",
    "              'ViT (Full, t-SNE)', 'ResNet (Full, t-SNE)', 'MobileViT (Full, t-SNE)',\n",
    "              'ViT (Imbalanced, PCA)', 'ResNet (Imbalanced, PCA)', 'MobileViT (Imbalanced, PCA)',\n",
    "              'ViT (Imbalanced, t-SNE)', 'ResNet (Imbalanced, t-SNE)', 'MobileViT (Imbalanced, t-SNE)'],\n",
    "    'Intra-class Distance': [\n",
    "        metrics['vit_full_pca']['avg_intra_class_distance'],\n",
    "        metrics['resnet_full_pca']['avg_intra_class_distance'],\n",
    "        metrics['mobilevit_full_pca']['avg_intra_class_distance'],\n",
    "        metrics['vit_full_tsne']['avg_intra_class_distance'],\n",
    "        metrics['resnet_full_tsne']['avg_intra_class_distance'],\n",
    "        metrics['mobilevit_full_tsne']['avg_intra_class_distance'],\n",
    "        metrics['vit_imbalanced_pca']['avg_intra_class_distance'],\n",
    "        metrics['resnet_imbalanced_pca']['avg_intra_class_distance'],\n",
    "        metrics['mobilevit_imbalanced_pca']['avg_intra_class_distance'],\n",
    "        metrics['vit_imbalanced_tsne']['avg_intra_class_distance'],\n",
    "        metrics['resnet_imbalanced_tsne']['avg_intra_class_distance'],\n",
    "        metrics['mobilevit_imbalanced_tsne']['avg_intra_class_distance']\n",
    "    ],\n",
    "    'Inter-class Distance': [\n",
    "        metrics['vit_full_pca']['avg_inter_class_distance'],\n",
    "        metrics['resnet_full_pca']['avg_inter_class_distance'],\n",
    "        metrics['mobilevit_full_pca']['avg_inter_class_distance'],\n",
    "        metrics['vit_full_tsne']['avg_inter_class_distance'],\n",
    "        metrics['resnet_full_tsne']['avg_inter_class_distance'],\n",
    "        metrics['mobilevit_full_tsne']['avg_inter_class_distance'],\n",
    "        metrics['vit_imbalanced_pca']['avg_inter_class_distance'],\n",
    "        metrics['resnet_imbalanced_pca']['avg_inter_class_distance'],\n",
    "        metrics['mobilevit_imbalanced_pca']['avg_inter_class_distance'],\n",
    "        metrics['vit_imbalanced_tsne']['avg_inter_class_distance'],\n",
    "        metrics['resnet_imbalanced_tsne']['avg_inter_class_distance'],\n",
    "        metrics['mobilevit_imbalanced_tsne']['avg_inter_class_distance']\n",
    "    ],\n",
    "    'Separation Ratio': [\n",
    "        metrics['vit_full_pca']['separation_ratio'],\n",
    "        metrics['resnet_full_pca']['separation_ratio'],\n",
    "        metrics['mobilevit_full_pca']['separation_ratio'],\n",
    "        metrics['vit_full_tsne']['separation_ratio'],\n",
    "        metrics['resnet_full_tsne']['separation_ratio'],\n",
    "        metrics['mobilevit_full_tsne']['separation_ratio'],\n",
    "        metrics['vit_imbalanced_pca']['separation_ratio'],\n",
    "        metrics['resnet_imbalanced_pca']['separation_ratio'],\n",
    "        metrics['mobilevit_imbalanced_pca']['separation_ratio'],\n",
    "        metrics['vit_imbalanced_tsne']['separation_ratio'],\n",
    "        metrics['resnet_imbalanced_tsne']['separation_ratio'],\n",
    "        metrics['mobilevit_imbalanced_tsne']['separation_ratio']\n",
    "    ]\n",
    "})\n",
    "\n",
    "# print(\"\\nFeature Separation Metrics:\")\n",
    "# print(metrics_df)\n",
    "\n",
    "metrics_df.to_csv(os.path.join(output_dir, 'imbalance_feature_separation_metrics.csv'), index=False)\n",
    "print(f\"Saved feature separation metrics to {os.path.join(output_dir, 'imbalance_feature_separation_metrics.csv')}\")\n",
    "\n",
    "# Plot separation metrics as a bar chart for comparison\n",
    "plt.figure(figsize=(20, 10))\n",
    "metrics_df_plot = pd.melt(metrics_df, id_vars=['Model'], \n",
    "                          value_vars=['Intra-class Distance', 'Inter-class Distance', 'Separation Ratio'],\n",
    "                          var_name='Metric', value_name='Value')\n",
    "\n",
    "\n",
    "ax = sns.barplot(x='Model', y='Value', hue='Metric', data=metrics_df_plot)\n",
    "plt.title('Feature Space Separation Metrics Comparison', fontsize=18)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'imbalance_separation_metrics_comparison.png'))\n",
    "plt.close()\n",
    "\n",
    "vit_metrics = metrics_df[metrics_df['Model'].str.contains('ViT(?! Mobile)')]\n",
    "resnet_metrics = metrics_df[metrics_df['Model'].str.contains('ResNet')]\n",
    "mobilevit_metrics = metrics_df[metrics_df['Model'].str.contains('MobileViT')]\n",
    "pca_metrics = metrics_df[metrics_df['Model'].str.contains('PCA')]\n",
    "tsne_metrics = metrics_df[metrics_df['Model'].str.contains('t-SNE')]\n",
    "\n",
    "# Comparisons by model type\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 24))\n",
    "\n",
    "# ViT comparison\n",
    "vit_metrics_plot = pd.melt(vit_metrics, id_vars=['Model'], \n",
    "                          value_vars=['Intra-class Distance', 'Inter-class Distance', 'Separation Ratio'],\n",
    "                          var_name='Metric', value_name='Value')\n",
    "sns.barplot(x='Model', y='Value', hue='Metric', data=vit_metrics_plot, ax=axes[0])\n",
    "axes[0].set_title('ViT Feature Space Metrics: Full vs Imbalanced', fontsize=16)\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45, ha='right')\n",
    "axes[0].legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# ResNet comparison\n",
    "resnet_metrics_plot = pd.melt(resnet_metrics, id_vars=['Model'], \n",
    "                            value_vars=['Intra-class Distance', 'Inter-class Distance', 'Separation Ratio'],\n",
    "                            var_name='Metric', value_name='Value')\n",
    "sns.barplot(x='Model', y='Value', hue='Metric', data=resnet_metrics_plot, ax=axes[1])\n",
    "axes[1].set_title('ResNet Feature Space Metrics: Full vs Imbalanced', fontsize=16)\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45, ha='right')\n",
    "axes[1].legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# MobileViT comparison\n",
    "mobilevit_metrics_plot = pd.melt(mobilevit_metrics, id_vars=['Model'], \n",
    "                                value_vars=['Intra-class Distance', 'Inter-class Distance', 'Separation Ratio'],\n",
    "                                var_name='Metric', value_name='Value')\n",
    "sns.barplot(x='Model', y='Value', hue='Metric', data=mobilevit_metrics_plot, ax=axes[2])\n",
    "axes[2].set_title('MobileViT Feature Space Metrics: Full vs Imbalanced', fontsize=16)\n",
    "axes[2].set_xticklabels(axes[2].get_xticklabels(), rotation=45, ha='right')\n",
    "axes[2].legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'model_comparison_metrics.png'))\n",
    "plt.close()\n",
    "\n",
    "# Calculate minority class-specific metrics\n",
    "def calculate_minority_class_metrics(features, labels, min_class):\n",
    "    \"\"\"\n",
    "    Calculate metrics specific to the minority class:\n",
    "    - Distance from minority class center to other class centers\n",
    "    - Compactness of minority class (average distance from points to center)\n",
    "    \"\"\"\n",
    "    all_classes = np.unique(labels)\n",
    "    \n",
    "    \n",
    "    centers = {}\n",
    "    for c in all_classes:\n",
    "        centers[c] = np.mean(features[labels == c], axis=0)\n",
    "    \n",
    "    \n",
    "    min_features = features[labels == min_class]\n",
    "    min_center = centers[min_class]\n",
    "    min_distances = np.sqrt(np.sum((min_features - min_center)**2, axis=1))\n",
    "    min_compactness = np.mean(min_distances) if len(min_distances) > 0 else 0.0\n",
    "    \n",
    "    \n",
    "    distances_to_other_centers = []\n",
    "    for c in all_classes:\n",
    "        if c != min_class:\n",
    "            dist = np.sqrt(np.sum((centers[min_class] - centers[c])**2))\n",
    "            distances_to_other_centers.append(dist)\n",
    "    \n",
    "    avg_distance_to_others = np.mean(distances_to_other_centers) if distances_to_other_centers else 0.0\n",
    "    min_separation = avg_distance_to_others / min_compactness if min_compactness != 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'minority_compactness': min_compactness,\n",
    "        'avg_distance_to_others': avg_distance_to_others,\n",
    "        'minority_separation_ratio': min_separation\n",
    "    }\n",
    "\n",
    "# Minority class metrics\n",
    "minority_metrics = {}\n",
    "\n",
    "# Full dataset\n",
    "minority_metrics['vit_full_pca'] = calculate_minority_class_metrics(vit_pca_full_result, vit_labels_full, min_class)\n",
    "minority_metrics['resnet_full_pca'] = calculate_minority_class_metrics(resnet_pca_full_result, resnet_labels_full, min_class)\n",
    "minority_metrics['mobilevit_full_pca'] = calculate_minority_class_metrics(mobilevit_pca_full_result, mobilevit_labels_full, min_class)\n",
    "minority_metrics['vit_full_tsne'] = calculate_minority_class_metrics(vit_tsne_full_result, vit_labels_full, min_class)\n",
    "minority_metrics['resnet_full_tsne'] = calculate_minority_class_metrics(resnet_tsne_full_result, resnet_labels_full, min_class)\n",
    "minority_metrics['mobilevit_full_tsne'] = calculate_minority_class_metrics(mobilevit_tsne_full_result, mobilevit_labels_full, min_class)\n",
    "\n",
    "# Imbalanced dataset\n",
    "minority_metrics['vit_imbalanced_pca'] = calculate_minority_class_metrics(vit_pca_imbalanced_result, vit_labels_imbalanced, min_class)\n",
    "minority_metrics['resnet_imbalanced_pca'] = calculate_minority_class_metrics(resnet_pca_imbalanced_result, resnet_labels_imbalanced, min_class)\n",
    "minority_metrics['mobilevit_imbalanced_pca'] = calculate_minority_class_metrics(mobilevit_pca_imbalanced_result, mobilevit_labels_imbalanced, min_class)\n",
    "minority_metrics['vit_imbalanced_tsne'] = calculate_minority_class_metrics(vit_tsne_imbalanced_result, vit_labels_imbalanced, min_class)\n",
    "minority_metrics['resnet_imbalanced_tsne'] = calculate_minority_class_metrics(resnet_tsne_imbalanced_result, resnet_labels_imbalanced, min_class)\n",
    "minority_metrics['mobilevit_imbalanced_tsne'] = calculate_minority_class_metrics(mobilevit_tsne_imbalanced_result, mobilevit_labels_imbalanced, min_class)\n",
    "\n",
    "# Minority class metrics dataframe\n",
    "minority_metrics_df = pd.DataFrame({\n",
    "    'Model': ['ViT (Full, PCA)', 'ResNet (Full, PCA)', 'MobileViT (Full, PCA)', \n",
    "              'ViT (Full, t-SNE)', 'ResNet (Full, t-SNE)', 'MobileViT (Full, t-SNE)',\n",
    "              'ViT (Imbalanced, PCA)', 'ResNet (Imbalanced, PCA)', 'MobileViT (Imbalanced, PCA)',\n",
    "              'ViT (Imbalanced, t-SNE)', 'ResNet (Imbalanced, t-SNE)', 'MobileViT (Imbalanced, t-SNE)'],\n",
    "    'Minority Compactness': [\n",
    "        minority_metrics['vit_full_pca']['minority_compactness'],\n",
    "        minority_metrics['resnet_full_pca']['minority_compactness'],\n",
    "        minority_metrics['mobilevit_full_pca']['minority_compactness'],\n",
    "        minority_metrics['vit_full_tsne']['minority_compactness'],\n",
    "        minority_metrics['resnet_full_tsne']['minority_compactness'],\n",
    "        minority_metrics['mobilevit_full_tsne']['minority_compactness'],\n",
    "        minority_metrics['vit_imbalanced_pca']['minority_compactness'],\n",
    "        minority_metrics['resnet_imbalanced_pca']['minority_compactness'],\n",
    "        minority_metrics['mobilevit_imbalanced_pca']['minority_compactness'],\n",
    "        minority_metrics['vit_imbalanced_tsne']['minority_compactness'],\n",
    "        minority_metrics['resnet_imbalanced_tsne']['minority_compactness'],\n",
    "        minority_metrics['mobilevit_imbalanced_tsne']['minority_compactness']\n",
    "    ],\n",
    "    'Avg Distance to Others': [\n",
    "        minority_metrics['vit_full_pca']['avg_distance_to_others'],\n",
    "        minority_metrics['resnet_full_pca']['avg_distance_to_others'],\n",
    "        minority_metrics['mobilevit_full_pca']['avg_distance_to_others'],\n",
    "        minority_metrics['vit_full_tsne']['avg_distance_to_others'],\n",
    "        minority_metrics['resnet_full_tsne']['avg_distance_to_others'],\n",
    "        minority_metrics['mobilevit_full_tsne']['avg_distance_to_others'],\n",
    "        minority_metrics['vit_imbalanced_pca']['avg_distance_to_others'],\n",
    "        minority_metrics['resnet_imbalanced_pca']['avg_distance_to_others'],\n",
    "        minority_metrics['mobilevit_imbalanced_pca']['avg_distance_to_others'],\n",
    "        minority_metrics['vit_imbalanced_tsne']['avg_distance_to_others'],\n",
    "        minority_metrics['resnet_imbalanced_tsne']['avg_distance_to_others'],\n",
    "        minority_metrics['mobilevit_imbalanced_tsne']['avg_distance_to_others']\n",
    "    ],\n",
    "    'Minority Separation Ratio': [\n",
    "        minority_metrics['vit_full_pca']['minority_separation_ratio'],\n",
    "        minority_metrics['resnet_full_pca']['minority_separation_ratio'],\n",
    "        minority_metrics['mobilevit_full_pca']['minority_separation_ratio'],\n",
    "        minority_metrics['vit_full_tsne']['minority_separation_ratio'],\n",
    "        minority_metrics['resnet_full_tsne']['minority_separation_ratio'],\n",
    "        minority_metrics['mobilevit_full_tsne']['minority_separation_ratio'],\n",
    "        minority_metrics['vit_imbalanced_pca']['minority_separation_ratio'],\n",
    "        minority_metrics['resnet_imbalanced_pca']['minority_separation_ratio'],\n",
    "        minority_metrics['mobilevit_imbalanced_pca']['minority_separation_ratio'],\n",
    "        minority_metrics['vit_imbalanced_tsne']['minority_separation_ratio'],\n",
    "        minority_metrics['resnet_imbalanced_tsne']['minority_separation_ratio'],\n",
    "        minority_metrics['mobilevit_imbalanced_tsne']['minority_separation_ratio']\n",
    "    ]\n",
    "})\n",
    "\n",
    "# print(\"\\nMinority Class Metrics:\")\n",
    "# print(minority_metrics_df)\n",
    "\n",
    "minority_metrics_df.to_csv(os.path.join(output_dir, 'minority_class_metrics.csv'), index=False)\n",
    "print(f\"Saved minority class metrics to {os.path.join(output_dir, 'minority_class_metrics.csv')}\")\n",
    "\n",
    "# Plot minority class metrics\n",
    "plt.figure(figsize=(20, 10))\n",
    "minority_metrics_plot = pd.melt(minority_metrics_df, id_vars=['Model'], \n",
    "                              value_vars=['Minority Compactness', 'Avg Distance to Others', 'Minority Separation Ratio'],\n",
    "                              var_name='Metric', value_name='Value')\n",
    "\n",
    "sns.barplot(x='Model', y='Value', hue='Metric', data=minority_metrics_plot)\n",
    "plt.title('Minority Class Metrics Comparison', fontsize=18)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'minority_class_metrics_comparison.png'))\n",
    "plt.close()\n",
    "\n",
    "# Imbalance impact\n",
    "imbalance_impact = pd.DataFrame({\n",
    "    'Model': ['ViT (PCA)', 'ResNet (PCA)', 'MobileViT (PCA)', 'ViT (t-SNE)', 'ResNet (t-SNE)', 'MobileViT (t-SNE)'],\n",
    "    'Separation Ratio Change (%)': [\n",
    "        (metrics['vit_imbalanced_pca']['separation_ratio'] / metrics['vit_full_pca']['separation_ratio'] - 1) * 100,\n",
    "        (metrics['resnet_imbalanced_pca']['separation_ratio'] / metrics['resnet_full_pca']['separation_ratio'] - 1) * 100,\n",
    "        (metrics['mobilevit_imbalanced_pca']['separation_ratio'] / metrics['mobilevit_full_pca']['separation_ratio'] - 1) * 100,\n",
    "        (metrics['vit_imbalanced_tsne']['separation_ratio'] / metrics['vit_full_tsne']['separation_ratio'] - 1) * 100,\n",
    "        (metrics['resnet_imbalanced_tsne']['separation_ratio'] / metrics['resnet_full_tsne']['separation_ratio'] - 1) * 100,\n",
    "        (metrics['mobilevit_imbalanced_tsne']['separation_ratio'] / metrics['mobilevit_full_tsne']['separation_ratio'] - 1) * 100\n",
    "    ],\n",
    "    'Minority Separation Change (%)': [\n",
    "        (minority_metrics['vit_imbalanced_pca']['minority_separation_ratio'] / \n",
    "         minority_metrics['vit_full_pca']['minority_separation_ratio'] - 1) * 100 if minority_metrics['vit_full_pca']['minority_separation_ratio'] != 0 else 0.0,\n",
    "        (minority_metrics['resnet_imbalanced_pca']['minority_separation_ratio'] / \n",
    "         minority_metrics['resnet_full_pca']['minority_separation_ratio'] - 1) * 100 if minority_metrics['resnet_full_pca']['minority_separation_ratio'] != 0 else 0.0,\n",
    "        (minority_metrics['mobilevit_imbalanced_pca']['minority_separation_ratio'] / \n",
    "         minority_metrics['mobilevit_full_pca']['minority_separation_ratio'] - 1) * 100 if minority_metrics['mobilevit_full_pca']['minority_separation_ratio'] != 0 else 0.0,\n",
    "        (minority_metrics['vit_imbalanced_tsne']['minority_separation_ratio'] / \n",
    "         minority_metrics['vit_full_tsne']['minority_separation_ratio'] - 1) * 100 if minority_metrics['vit_full_tsne']['minority_separation_ratio'] != 0 else 0.0,\n",
    "        (minority_metrics['resnet_imbalanced_tsne']['minority_separation_ratio'] / \n",
    "         minority_metrics['resnet_full_tsne']['minority_separation_ratio'] - 1) * 100 if minority_metrics['resnet_full_tsne']['minority_separation_ratio'] != 0 else 0.0,\n",
    "        (minority_metrics['mobilevit_imbalanced_tsne']['minority_separation_ratio'] / \n",
    "         minority_metrics['mobilevit_full_tsne']['minority_separation_ratio'] - 1) * 100 if minority_metrics['mobilevit_full_tsne']['minority_separation_ratio'] != 0 else 0.0\n",
    "    ]\n",
    "})\n",
    "\n",
    "# print(\"\\nImbalance Impact (% change from full to imbalanced dataset):\")\n",
    "# print(imbalance_impact)\n",
    "\n",
    "imbalance_impact.to_csv(os.path.join(output_dir, 'imbalance_impact.csv'), index=False)\n",
    "print(f\"Saved imbalance impact analysis to {os.path.join(output_dir, 'imbalance_impact.csv')}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(16, 8))\n",
    "imbalance_impact_plot = pd.melt(imbalance_impact, id_vars=['Model'], \n",
    "                              value_vars=['Separation Ratio Change (%)', 'Minority Separation Change (%)'],\n",
    "                              var_name='Metric', value_name='Percent Change')\n",
    "\n",
    "sns.barplot(x='Model', y='Percent Change', hue='Metric', data=imbalance_impact_plot)\n",
    "plt.title('Impact of Class Imbalance on Feature Space Metrics', fontsize=18)\n",
    "plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
    "plt.ylabel('Percent Change (%)')\n",
    "plt.legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'imbalance_impact.png'))\n",
    "plt.close()\n",
    "\n",
    "# Comparing ViT, ResNet, and MobileViT directly in terms of handling imbalance\n",
    "model_comparison = pd.DataFrame({\n",
    "    'Technique': ['PCA', 't-SNE'],\n",
    "    'ViT Overall Separation Ratio': [\n",
    "        metrics['vit_imbalanced_pca']['separation_ratio'],\n",
    "        metrics['vit_imbalanced_tsne']['separation_ratio']\n",
    "    ],\n",
    "    'ResNet Overall Separation Ratio': [\n",
    "        metrics['resnet_imbalanced_pca']['separation_ratio'],\n",
    "        metrics['resnet_imbalanced_tsne']['separation_ratio']\n",
    "    ],\n",
    "    'MobileViT Overall Separation Ratio': [\n",
    "        metrics['mobilevit_imbalanced_pca']['separation_ratio'],\n",
    "        metrics['mobilevit_imbalanced_tsne']['separation_ratio']\n",
    "    ],\n",
    "    'ViT Minority Separation Ratio': [\n",
    "        minority_metrics['vit_imbalanced_pca']['minority_separation_ratio'],\n",
    "        minority_metrics['vit_imbalanced_tsne']['minority_separation_ratio']\n",
    "    ],\n",
    "    'ResNet Minority Separation Ratio': [\n",
    "        minority_metrics['resnet_imbalanced_pca']['minority_separation_ratio'],\n",
    "        minority_metrics['resnet_imbalanced_tsne']['minority_separation_ratio']\n",
    "    ],\n",
    "    'MobileViT Minority Separation Ratio': [\n",
    "        minority_metrics['mobilevit_imbalanced_pca']['minority_separation_ratio'],\n",
    "        minority_metrics['mobilevit_imbalanced_tsne']['minority_separation_ratio']\n",
    "    ],\n",
    "    'ViT Imbalance Impact (%)': [\n",
    "        (metrics['vit_imbalanced_pca']['separation_ratio'] / metrics['vit_full_pca']['separation_ratio'] - 1) * 100,\n",
    "        (metrics['vit_imbalanced_tsne']['separation_ratio'] / metrics['vit_full_tsne']['separation_ratio'] - 1) * 100\n",
    "    ],\n",
    "    'ResNet Imbalance Impact (%)': [\n",
    "        (metrics['resnet_imbalanced_pca']['separation_ratio'] / metrics['resnet_full_pca']['separation_ratio'] - 1) * 100,\n",
    "        (metrics['resnet_imbalanced_tsne']['separation_ratio'] / metrics['resnet_full_tsne']['separation_ratio'] - 1) * 100\n",
    "    ],\n",
    "    'MobileViT Imbalance Impact (%)': [\n",
    "        (metrics['mobilevit_imbalanced_pca']['separation_ratio'] / metrics['mobilevit_full_pca']['separation_ratio'] - 1) * 100,\n",
    "        (metrics['mobilevit_imbalanced_tsne']['separation_ratio'] / metrics['mobilevit_full_tsne']['separation_ratio'] - 1) * 100\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\nDirectly Comparing ViT, ResNet, and MobileViT on Imbalanced Data:\")\n",
    "print(model_comparison)\n",
    "\n",
    "model_comparison.to_csv(os.path.join(output_dir, 'vit_resnet_mobilevit_imbalance.csv'), index=False)\n",
    "print(f\"Saved ViT, ResNet, and MobileViT comparison to {os.path.join(output_dir, 'vit_resnet_mobilevit_imbalance.csv')}\")\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(x='Technique', y='value', hue='variable', \n",
    "           data=pd.melt(model_comparison, id_vars=['Technique'], \n",
    "                      value_vars=['ViT Imbalance Impact (%)', 'ResNet Imbalance Impact (%)', 'MobileViT Imbalance Impact (%)']))\n",
    "plt.title('ViT vs ResNet vs MobileViT: Impact of Class Imbalance on Separation', fontsize=18)\n",
    "plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
    "plt.ylabel('Percent Change in Separation Ratio (%)')\n",
    "plt.legend(title='Model')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'vit_resnet_mobilevit_imbalance_impact.png'))\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(18, 16))\n",
    "\n",
    "comprehensive_df = pd.DataFrame({\n",
    "    'Model': ['ViT', 'ResNet', 'MobileViT', 'ViT', 'ResNet', 'MobileViT'],\n",
    "    'Dataset': ['Full', 'Full', 'Full', 'Imbalanced', 'Imbalanced', 'Imbalanced'],\n",
    "    'Overall Separation (PCA)': [\n",
    "        metrics['vit_full_pca']['separation_ratio'],\n",
    "        metrics['resnet_full_pca']['separation_ratio'],\n",
    "        metrics['mobilevit_full_pca']['separation_ratio'],\n",
    "        metrics['vit_imbalanced_pca']['separation_ratio'],\n",
    "        metrics['resnet_imbalanced_pca']['separation_ratio'],\n",
    "        metrics['mobilevit_imbalanced_pca']['separation_ratio']\n",
    "    ],\n",
    "    'Overall Separation (t-SNE)': [\n",
    "        metrics['vit_full_tsne']['separation_ratio'],\n",
    "        metrics['resnet_full_tsne']['separation_ratio'],\n",
    "        metrics['mobilevit_full_tsne']['separation_ratio'],\n",
    "        metrics['vit_imbalanced_tsne']['separation_ratio'],\n",
    "        metrics['resnet_imbalanced_tsne']['separation_ratio'],\n",
    "        metrics['mobilevit_imbalanced_tsne']['separation_ratio']\n",
    "    ],\n",
    "    'Minority Separation (PCA)': [\n",
    "        minority_metrics['vit_full_pca']['minority_separation_ratio'],\n",
    "        minority_metrics['resnet_full_pca']['minority_separation_ratio'],\n",
    "        minority_metrics['mobilevit_full_pca']['minority_separation_ratio'],\n",
    "        minority_metrics['vit_imbalanced_pca']['minority_separation_ratio'],\n",
    "        minority_metrics['resnet_imbalanced_pca']['minority_separation_ratio'],\n",
    "        minority_metrics['mobilevit_imbalanced_pca']['minority_separation_ratio']\n",
    "    ],\n",
    "    'Minority Separation (t-SNE)': [\n",
    "        minority_metrics['vit_full_tsne']['minority_separation_ratio'],\n",
    "        minority_metrics['resnet_full_tsne']['minority_separation_ratio'],\n",
    "        minority_metrics['mobilevit_full_tsne']['minority_separation_ratio'],\n",
    "        minority_metrics['vit_imbalanced_tsne']['minority_separation_ratio'],\n",
    "        minority_metrics['resnet_imbalanced_tsne']['minority_separation_ratio'],\n",
    "        minority_metrics['mobilevit_imbalanced_tsne']['minority_separation_ratio']\n",
    "    ]\n",
    "})\n",
    "\n",
    "plot_data = pd.melt(comprehensive_df, \n",
    "                  id_vars=['Model', 'Dataset'], \n",
    "                  value_vars=['Overall Separation (PCA)', 'Overall Separation (t-SNE)', \n",
    "                            'Minority Separation (PCA)', 'Minority Separation (t-SNE)'],\n",
    "                  var_name='Metric', value_name='Value')\n",
    "\n",
    "g = sns.catplot(\n",
    "    data=plot_data, kind=\"bar\",\n",
    "    x=\"Dataset\", y=\"Value\", hue=\"Model\", col=\"Metric\",\n",
    "    height=5, aspect=0.8, sharey=False, col_wrap=2\n",
    ")\n",
    "\n",
    "g.fig.suptitle('Comprehensive Feature Space Analysis: ViT vs ResNet vs MobileViT', fontsize=20, y=1.05)\n",
    "g.set_titles(col_template=\"{col_name}\")\n",
    "g.set_axis_labels(\"Dataset\", \"Separation Ratio\")\n",
    "g.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'comprehensive_comparison.png'))\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\nAll visualizations and analyses saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01dcba6",
   "metadata": {},
   "source": [
    "# Class Imbalance and OOD Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b032da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "vit_bundle_path = \"expt_1/vit_tiny_model_bundle.pth\"\n",
    "resnet_bundle_path = \"expt_1/resnet50_model_bundle.pth\"\n",
    "mobilevit_bundle_path = \"expt_1/mobilevit_xs_model_bundle.pth\"\n",
    "data_dir = \"NSD/test\"\n",
    "output_dir = \"expt_1/imbalance_ood_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def set_seed(seed=48):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()\n",
    "\n",
    "def load_model_bundle(bundle_path, model_name):\n",
    "    bundle = torch.load(bundle_path, map_location=\"cpu\", weights_only=False)\n",
    "    if bundle[\"model_name\"] != model_name:\n",
    "        raise ValueError(f\"Expected {model_name}, got {bundle['model_name']}\")\n",
    "    if \"resnet\" in model_name:\n",
    "        model = models.resnet50(weights=None)\n",
    "        model.fc = nn.Linear(model.fc.in_features, bundle[\"num_classes\"])\n",
    "    else:\n",
    "        model = timm.create_model(model_name, pretrained=False, num_classes=bundle[\"num_classes\"])\n",
    "    model.load_state_dict(bundle[\"state_dict\"])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model, bundle[\"transform\"]\n",
    "\n",
    "print(\"Loading ViT model...\")\n",
    "vit_model, vit_transform = load_model_bundle(vit_bundle_path, \"vit_tiny_patch16_224\")\n",
    "print(\"Loading ResNet model...\")\n",
    "resnet_model, resnet_transform = load_model_bundle(resnet_bundle_path, \"resnet50\")\n",
    "print(\"Loading MobileViT model...\")\n",
    "mobilevit_model, mobilevit_transform = load_model_bundle(mobilevit_bundle_path, \"mobilevit_xs\")\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "dataset = datasets.ImageFolder(data_dir, transform=vit_transform)\n",
    "class_names = dataset.classes\n",
    "class_counts = np.bincount([label for _, label in dataset.samples])\n",
    "print(\"Class distribution:\", dict(zip(class_names, class_counts)))\n",
    "\n",
    "print(\"Creating imbalanced dataset...\")\n",
    "min_class = np.argmin(class_counts)\n",
    "print(f\"Minority class: {class_names[min_class]} with {class_counts[min_class]} samples\")\n",
    "min_indices = [i for i, (_, lbl) in enumerate(dataset.samples) if lbl == min_class]\n",
    "min_keep_indices = np.random.choice(min_indices, size=len(min_indices) // 2, replace=False)\n",
    "other_indices = [i for i, (_, lbl) in enumerate(dataset.samples) if lbl != min_class]\n",
    "imbalanced_indices = other_indices + list(min_keep_indices)\n",
    "imbalanced_dataset = Subset(dataset, imbalanced_indices)\n",
    "\n",
    "print(\"Creating OOD datasets...\")\n",
    "ood_class = 0\n",
    "print(f\"OOD class: {class_names[ood_class]}\")\n",
    "ood_train_indices = [i for i, (_, lbl) in enumerate(dataset.samples) if lbl != ood_class]\n",
    "ood_test_indices = [i for i, (_, lbl) in enumerate(dataset.samples) if lbl == ood_class]\n",
    "ood_train_dataset = Subset(dataset, ood_train_indices)\n",
    "ood_test_dataset = Subset(dataset, ood_test_indices)\n",
    "\n",
    "batch_size = 32\n",
    "full_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "imbalanced_loader = DataLoader(imbalanced_dataset, batch_size=batch_size, shuffle=False)\n",
    "ood_train_loader = DataLoader(ood_train_dataset, batch_size=batch_size, shuffle=False)\n",
    "ood_test_loader = DataLoader(ood_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "def plot_class_distribution(dataset_subset, title, save_path=None):\n",
    "    if isinstance(dataset_subset, Subset):\n",
    "        labels = [dataset_subset.dataset.targets[i] for i in dataset_subset.indices]\n",
    "    else:\n",
    "        labels = [y for _, y in dataset_subset.samples]\n",
    "    class_dist = np.bincount(labels, minlength=len(class_names))\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(len(class_names)), class_dist)\n",
    "    plt.xticks(range(len(class_names)), class_names, rotation=45)\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, save_path=None):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_model(model, dataloader, model_name, dataset_name):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=f\"Evaluating {model_name} on {dataset_name}\"):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    inference_time = time.time() - start_time\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(all_labels, all_preds) * 100,\n",
    "        \"precision\": precision_score(all_labels, all_preds, average=\"macro\", zero_division=0),\n",
    "        \"recall\": recall_score(all_labels, all_preds, average=\"macro\", zero_division=0),\n",
    "        \"f1\": f1_score(all_labels, all_preds, average=\"macro\", zero_division=0),\n",
    "        \"inference_time\": inference_time\n",
    "    }\n",
    "    print(f\"{model_name} on {dataset_name}:\")\n",
    "    for k, v in metrics.items():\n",
    "        unit = \"s\" if \"time\" in k else \"\"\n",
    "        print(f\"  {k.capitalize()}: {v:.2f}{unit}\")\n",
    "    plot_confusion_matrix(\n",
    "        all_labels, \n",
    "        all_preds, \n",
    "        f\"{model_name} - {dataset_name} Confusion Matrix\",\n",
    "        f\"{output_dir}/{model_name.lower()}_{dataset_name}_confusion_matrix.png\"\n",
    "    )\n",
    "    return metrics, all_preds, all_labels\n",
    "\n",
    "print(\"\\nClass distribution analysis:\")\n",
    "plot_class_distribution(dataset, \"Full Dataset Class Distribution\", f\"{output_dir}/full_class_distribution.png\")\n",
    "plot_class_distribution(imbalanced_dataset, \"Imbalanced Dataset Class Distribution\", f\"{output_dir}/imbalanced_class_distribution.png\")\n",
    "plot_class_distribution(ood_train_dataset, \"OOD Training Dataset Class Distribution\", f\"{output_dir}/ood_train_class_distribution.png\")\n",
    "plot_class_distribution(ood_test_dataset, \"OOD Test Dataset Class Distribution\", f\"{output_dir}/ood_test_class_distribution.png\")\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"\\n===== Full Dataset Evaluation =====\")\n",
    "results[\"vit_full\"] = evaluate_model(vit_model, full_loader, \"ViT\", \"Full Dataset\")\n",
    "results[\"resnet_full\"] = evaluate_model(resnet_model, full_loader, \"ResNet\", \"Full Dataset\")\n",
    "results[\"mobilevit_full\"] = evaluate_model(mobilevit_model, full_loader, \"MobileViT\", \"Full Dataset\")\n",
    "\n",
    "print(\"\\n===== Imbalanced Dataset Evaluation =====\")\n",
    "results[\"vit_imbalanced\"] = evaluate_model(vit_model, imbalanced_loader, \"ViT\", \"Imbalanced Dataset\")\n",
    "results[\"resnet_imbalanced\"] = evaluate_model(resnet_model, imbalanced_loader, \"ResNet\", \"Imbalanced Dataset\")\n",
    "results[\"mobilevit_imbalanced\"] = evaluate_model(mobilevit_model, imbalanced_loader, \"MobileViT\", \"Imbalanced Dataset\")\n",
    "\n",
    "print(\"\\n===== OOD Evaluation =====\")\n",
    "print(\"Training dataset (class excluded):\")\n",
    "results[\"vit_ood_train\"] = evaluate_model(vit_model, ood_train_loader, \"ViT\", \"OOD Train\")\n",
    "results[\"resnet_ood_train\"] = evaluate_model(resnet_model, ood_train_loader, \"ResNet\", \"OOD Train\")\n",
    "results[\"mobilevit_ood_train\"] = evaluate_model(mobilevit_model, ood_train_loader, \"MobileViT\", \"OOD Train\")\n",
    "\n",
    "print(\"Test dataset (excluded class only):\")\n",
    "results[\"vit_ood_test\"] = evaluate_model(vit_model, ood_test_loader, \"ViT\", \"OOD Test\")\n",
    "results[\"resnet_ood_test\"] = evaluate_model(resnet_model, ood_test_loader, \"ResNet\", \"OOD Test\")\n",
    "results[\"mobilevit_ood_test\"] = evaluate_model(mobilevit_model, ood_test_loader, \"MobileViT\", \"OOD Test\")\n",
    "\n",
    "summary = []\n",
    "for model_name in [\"ViT\", \"ResNet\", \"MobileViT\"]:\n",
    "    model_key = model_name.lower()\n",
    "    for dataset_name in [\"Full Dataset\", \"Imbalanced Dataset\", \"OOD Train\", \"OOD Test\"]:\n",
    "        dataset_key = dataset_name.lower().replace(\" \", \"_\")\n",
    "        key = f\"{model_key}_{dataset_key}\"\n",
    "        if key in results:\n",
    "            metrics, _, _ = results[key]\n",
    "            summary.append({\n",
    "                \"Model\": model_name,\n",
    "                \"Dataset\": dataset_name,\n",
    "                \"Accuracy\": metrics[\"accuracy\"],\n",
    "                \"Precision\": metrics[\"precision\"],\n",
    "                \"Recall\": metrics[\"recall\"],\n",
    "                \"F1 Score\": metrics[\"f1\"],\n",
    "                \"Inference Time\": metrics[\"inference_time\"]\n",
    "            })\n",
    "\n",
    "models = [\"ViT\", \"ResNet\", \"MobileViT\"]\n",
    "datasets = [\"Full Dataset\", \"Imbalanced Dataset\", \"OOD Train\", \"OOD Test\"]\n",
    "metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"Inference Time\"]\n",
    "\n",
    "plot_data = {}\n",
    "for metric in metrics:\n",
    "    plot_data[metric] = np.zeros((len(models), len(datasets)))\n",
    "    for i, model in enumerate(models):\n",
    "        for j, dataset in enumerate(datasets):\n",
    "            for entry in summary:\n",
    "                if entry[\"Model\"] == model and entry[\"Dataset\"] == dataset:\n",
    "                    plot_data[metric][i, j] = entry[metric]\n",
    "\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    x = np.arange(len(datasets))\n",
    "    width = 0.25\n",
    "    for i, model in enumerate(models):\n",
    "        offset = width * (i - 1)\n",
    "        plt.bar(x + offset, plot_data[metric][i], width, label=model)\n",
    "    plt.xlabel('Dataset')\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(f'Model Comparison - {metric}')\n",
    "    plt.xticks(x, datasets)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/comparison_{metric.lower().replace(' ', '_')}.png\")\n",
    "    plt.close()\n",
    "\n",
    "print(f\"All results saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4935e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, recall_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "import timm\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "vit_bundle_path = \"expt_1/vit_tiny_model_bundle.pth\"\n",
    "resnet_bundle_path = \"expt_1/resnet50_model_bundle.pth\"\n",
    "mobilevit_bundle_path = \"expt_1/mobilevit_xs_model_bundle.pth\"\n",
    "data_dir = \"NSD/test\"\n",
    "output_dir = \"expt_1/imbalance_ood_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def set_seed(seed=48):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()\n",
    "\n",
    "def load_model_bundle(bundle_path, model_name):\n",
    "    bundle = torch.load(bundle_path, map_location=\"cpu\", weights_only=False)\n",
    "    if bundle[\"model_name\"] != model_name:\n",
    "        raise ValueError(f\"Expected {model_name}, got {bundle['model_name']}\")\n",
    "    if \"resnet\" in model_name:\n",
    "        model = models.resnet50(weights=None)\n",
    "        model.fc = nn.Linear(model.fc.in_features, bundle[\"num_classes\"])\n",
    "    else:\n",
    "        model = timm.create_model(model_name, pretrained=False, num_classes=bundle[\"num_classes\"])\n",
    "    model.load_state_dict(bundle[\"state_dict\"])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model, bundle[\"transform\"]\n",
    "\n",
    "print(\"Loading ViT model...\")\n",
    "vit_model, vit_transform = load_model_bundle(vit_bundle_path, \"vit_tiny_patch16_224\")\n",
    "print(\"Loading ResNet model...\")\n",
    "resnet_model, resnet_transform = load_model_bundle(resnet_bundle_path, \"resnet50\")\n",
    "print(\"Loading MobileViT model...\")\n",
    "mobilevit_model, mobilevit_transform = load_model_bundle(mobilevit_bundle_path, \"mobilevit_xs\")\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "dataset = datasets.ImageFolder(data_dir, transform=vit_transform)\n",
    "class_names = dataset.classes\n",
    "class_counts = np.bincount([label for _, label in dataset.samples])\n",
    "print(\"Class distribution:\", dict(zip(class_names, class_counts)))\n",
    "\n",
    "print(\"Creating imbalanced dataset...\")\n",
    "min_class = np.argmin(class_counts)\n",
    "print(f\"Minority class: {class_names[min_class]} with {class_counts[min_class]} samples\")\n",
    "min_indices = [i for i, (_, lbl) in enumerate(dataset.samples) if lbl == min_class]\n",
    "min_keep_indices = np.random.choice(min_indices, size=len(min_indices) // 2, replace=False)\n",
    "other_indices = [i for i, (_, lbl) in enumerate(dataset.samples) if lbl != min_class]\n",
    "imbalanced_indices = other_indices + list(min_keep_indices)\n",
    "imbalanced_dataset = Subset(dataset, imbalanced_indices)\n",
    "\n",
    "print(\"Creating OOD datasets...\")\n",
    "ood_class = 0\n",
    "print(f\"OOD class: {class_names[ood_class]}\")\n",
    "ood_train_indices = [i for i, (_, lbl) in enumerate(dataset.samples) if lbl != ood_class]\n",
    "ood_test_indices = [i for i, (_, lbl) in enumerate(dataset.samples) if lbl == ood_class]\n",
    "ood_train_dataset = Subset(dataset, ood_train_indices)\n",
    "ood_test_dataset = Subset(dataset, ood_test_indices)\n",
    "\n",
    "batch_size = 32\n",
    "full_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "imbalanced_loader = DataLoader(imbalanced_dataset, batch_size=batch_size, shuffle=False)\n",
    "ood_train_loader = DataLoader(ood_train_dataset, batch_size=batch_size, shuffle=False)\n",
    "ood_test_loader = DataLoader(ood_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "def plot_class_distribution(dataset_subset, title, save_path=None):\n",
    "    if isinstance(dataset_subset, Subset):\n",
    "        labels = [dataset_subset.dataset.targets[i] for i in dataset_subset.indices]\n",
    "    else:\n",
    "        labels = [y for _, y in dataset_subset.samples]\n",
    "    class_dist = np.bincount(labels, minlength=len(class_names))\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(len(class_names)), class_dist)\n",
    "    plt.xticks(range(len(class_names)), class_names, rotation=45)\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, save_path=None):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_model(model, dataloader, model_name, dataset_name):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=f\"Evaluating {model_name} on {dataset_name}\"):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate metrics only if there are predictions\n",
    "    if len(all_preds) > 0:\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(all_labels, all_preds) * 100,\n",
    "            \"precision\": precision_score(all_labels, all_preds, average=\"macro\", zero_division=0),\n",
    "            \"recall\": recall_score(all_labels, all_preds, average=\"macro\", zero_division=0),\n",
    "            \"f1\": f1_score(all_labels, all_preds, average=\"macro\", zero_division=0),\n",
    "            \"inference_time\": inference_time\n",
    "        }\n",
    "    else:\n",
    "        metrics = {\n",
    "            \"accuracy\": 0,\n",
    "            \"precision\": 0,\n",
    "            \"recall\": 0,\n",
    "            \"f1\": 0,\n",
    "            \"inference_time\": 0\n",
    "        }\n",
    "    \n",
    "    print(f\"{model_name} on {dataset_name}:\")\n",
    "    for k, v in metrics.items():\n",
    "        unit = \"s\" if \"time\" in k else \"\"\n",
    "        print(f\"  {k.capitalize()}: {v:.2f}{unit}\")\n",
    "    \n",
    "    if len(all_preds) > 0:\n",
    "        plot_confusion_matrix(\n",
    "            all_labels, \n",
    "            all_preds, \n",
    "            f\"{model_name} - {dataset_name} Confusion Matrix\",\n",
    "            f\"{output_dir}/{model_name.lower()}_{dataset_name.lower().replace(' ', '_')}_confusion_matrix.png\"\n",
    "        )\n",
    "    \n",
    "    return metrics, all_preds, all_labels\n",
    "\n",
    "print(\"\\nClass distribution analysis:\")\n",
    "plot_class_distribution(dataset, \"Full Dataset Class Distribution\", f\"{output_dir}/full_class_distribution.png\")\n",
    "plot_class_distribution(imbalanced_dataset, \"Imbalanced Dataset Class Distribution\", f\"{output_dir}/imbalanced_class_distribution.png\")\n",
    "plot_class_distribution(ood_train_dataset, \"OOD Training Dataset Class Distribution\", f\"{output_dir}/ood_train_class_distribution.png\")\n",
    "plot_class_distribution(ood_test_dataset, \"OOD Test Dataset Class Distribution\", f\"{output_dir}/ood_test_class_distribution.png\")\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"\\n===== Full Dataset Evaluation =====\")\n",
    "results[\"vit_full_dataset\"] = evaluate_model(vit_model, full_loader, \"ViT\", \"Full Dataset\")\n",
    "results[\"resnet_full_dataset\"] = evaluate_model(resnet_model, full_loader, \"ResNet\", \"Full Dataset\")\n",
    "results[\"mobilevit_full_dataset\"] = evaluate_model(mobilevit_model, full_loader, \"MobileViT\", \"Full Dataset\")\n",
    "\n",
    "print(\"\\n===== Imbalanced Dataset Evaluation =====\")\n",
    "results[\"vit_imbalanced_dataset\"] = evaluate_model(vit_model, imbalanced_loader, \"ViT\", \"Imbalanced Dataset\")\n",
    "results[\"resnet_imbalanced_dataset\"] = evaluate_model(resnet_model, imbalanced_loader, \"ResNet\", \"Imbalanced Dataset\")\n",
    "results[\"mobilevit_imbalanced_dataset\"] = evaluate_model(mobilevit_model, imbalanced_loader, \"MobileViT\", \"Imbalanced Dataset\")\n",
    "\n",
    "print(\"\\n===== OOD Evaluation =====\")\n",
    "print(\"Training dataset (class excluded):\")\n",
    "results[\"vit_ood_train\"] = evaluate_model(vit_model, ood_train_loader, \"ViT\", \"OOD Train\")\n",
    "results[\"resnet_ood_train\"] = evaluate_model(resnet_model, ood_train_loader, \"ResNet\", \"OOD Train\")\n",
    "results[\"mobilevit_ood_train\"] = evaluate_model(mobilevit_model, ood_train_loader, \"MobileViT\", \"OOD Train\")\n",
    "\n",
    "print(\"Test dataset (excluded class only):\")\n",
    "results[\"vit_ood_test\"] = evaluate_model(vit_model, ood_test_loader, \"ViT\", \"OOD Test\")\n",
    "results[\"resnet_ood_test\"] = evaluate_model(resnet_model, ood_test_loader, \"ResNet\", \"OOD Test\")\n",
    "results[\"mobilevit_ood_test\"] = evaluate_model(mobilevit_model, ood_test_loader, \"MobileViT\", \"OOD Test\")\n",
    "\n",
    "# Fix: Create a proper summary from results\n",
    "summary = []\n",
    "for key, (metrics, _, _) in results.items():\n",
    "    model_name = key.split(\"_\")[0]\n",
    "    model_name = \"ViT\" if model_name == \"vit\" else \"ResNet\" if model_name == \"resnet\" else \"MobileViT\"\n",
    "    \n",
    "    # Parse dataset name from key\n",
    "    if \"full_dataset\" in key:\n",
    "        dataset_name = \"Full Dataset\"\n",
    "    elif \"imbalanced_dataset\" in key:\n",
    "        dataset_name = \"Imbalanced Dataset\"\n",
    "    elif \"ood_train\" in key:\n",
    "        dataset_name = \"OOD Train\"\n",
    "    elif \"ood_test\" in key:\n",
    "        dataset_name = \"OOD Test\"\n",
    "    else:\n",
    "        dataset_name = \"Unknown\"\n",
    "    \n",
    "    summary.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Dataset\": dataset_name,\n",
    "        \"Accuracy\": metrics[\"accuracy\"],\n",
    "        \"Precision\": metrics[\"precision\"],\n",
    "        \"Recall\": metrics[\"recall\"],\n",
    "        \"F1 Score\": metrics[\"f1\"],\n",
    "        \"Inference Time\": metrics[\"inference_time\"]\n",
    "    })\n",
    "\n",
    "models = [\"ViT\", \"ResNet\", \"MobileViT\"]\n",
    "datasets = [\"Full Dataset\", \"Imbalanced Dataset\", \"OOD Train\", \"OOD Test\"]\n",
    "metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"Inference Time\"]\n",
    "\n",
    "# Fix: Initialize plot_data properly\n",
    "plot_data = {}\n",
    "for metric in metrics:\n",
    "    plot_data[metric] = np.zeros((len(models), len(datasets)))\n",
    "\n",
    "# Fix: Populate plot_data correctly\n",
    "for entry in summary:\n",
    "    model_idx = models.index(entry[\"Model\"])\n",
    "    dataset_idx = datasets.index(entry[\"Dataset\"])\n",
    "    for metric in metrics:\n",
    "        plot_data[metric][model_idx, dataset_idx] = entry[metric]\n",
    "\n",
    "# Create bar plots for each metric\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    x = np.arange(len(datasets))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        offset = width * (i - 1)\n",
    "        plt.bar(x + offset, plot_data[metric][i], width, label=model)\n",
    "    \n",
    "    plt.xlabel('Dataset')\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(f'Model Comparison - {metric}')\n",
    "    plt.xticks(x, datasets)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/comparison_{metric.lower().replace(' ', '_')}.png\")\n",
    "    plt.close()\n",
    "\n",
    "print(f\"All results saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a2e38e",
   "metadata": {},
   "source": [
    "# Interactive Gradcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b4528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_bundle_path = \"expt_1/vit_tiny_model_bundle.pth\"\n",
    "resnet_bundle_path = \"expt_1/resnet50_model_bundle.pth\"\n",
    "mobilevit_bundle_path = \"expt_1/mobilevit_xs_model_bundle.pth\"\n",
    "data_dir = \"NSD/test\"\n",
    "output_dir = \"gradcam_visualization\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "class ResNetGradCAM:\n",
    "    def __init__(self, model, target_layer_name=\"layer4\"):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        \n",
    "        \n",
    "        if target_layer_name == \"layer4\":\n",
    "            self.target_layer = model.layer4\n",
    "        else:\n",
    "            raise ValueError(f\"Target layer {target_layer_name} not found in model\")\n",
    "        \n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        \n",
    "        \n",
    "        self.hooks = []\n",
    "        self.register_hooks()\n",
    "        \n",
    "    def register_hooks(self):\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output\n",
    "            \n",
    "        def backward_hook(module, grad_input, grad_output):\n",
    "            self.gradients = grad_output[0]\n",
    "        \n",
    "        self.hooks.append(self.target_layer.register_forward_hook(forward_hook))\n",
    "        self.hooks.append(self.target_layer.register_full_backward_hook(backward_hook))\n",
    "        \n",
    "    def remove_hooks(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "        \n",
    "    def generate_cam(self, input_image, target_class=None):\n",
    "        model_output = self.model(input_image)\n",
    "        \n",
    "        if target_class is None:\n",
    "            target_class = torch.argmax(model_output, dim=1).item()\n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        one_hot = torch.zeros_like(model_output)\n",
    "        one_hot[0, target_class] = 1\n",
    "        model_output.backward(gradient=one_hot, retain_graph=True)\n",
    "        \n",
    "        gradients = self.gradients.detach().cpu().numpy()[0]\n",
    "        activations = self.activations.detach().cpu().numpy()[0]\n",
    "        weights = np.mean(gradients, axis=(1, 2))\n",
    "        \n",
    "        cam = np.zeros(activations.shape[1:], dtype=np.float32)\n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * activations[i]\n",
    "        \n",
    "        cam = np.maximum(cam, 0)\n",
    "        if np.max(cam) > 0:\n",
    "            cam = cam / np.max(cam)\n",
    "        \n",
    "        cam = cv2.resize(cam, (input_image.shape[3], input_image.shape[2]))\n",
    "        return cam, target_class\n",
    "\n",
    "\n",
    "class ViTGradCAM:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.target_layer = model.blocks[-1].norm1\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        self.hooks = []\n",
    "        self.register_hooks()\n",
    "        \n",
    "    def register_hooks(self):\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output\n",
    "            \n",
    "        def backward_hook(module, grad_input, grad_output):\n",
    "            self.gradients = grad_output[0]\n",
    "        \n",
    "        self.hooks.append(self.target_layer.register_forward_hook(forward_hook))\n",
    "        self.hooks.append(self.target_layer.register_full_backward_hook(backward_hook))\n",
    "        \n",
    "    def remove_hooks(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "        \n",
    "    def generate_cam(self, input_image, target_class=None):\n",
    "        model_output = self.model(input_image)\n",
    "        \n",
    "        if target_class is None:\n",
    "            target_class = torch.argmax(model_output, dim=1).item()\n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        one_hot = torch.zeros_like(model_output)\n",
    "        one_hot[0, target_class] = 1\n",
    "        model_output.backward(gradient=one_hot, retain_graph=True)\n",
    "        \n",
    "        gradients = self.gradients.detach().cpu().numpy()[0]\n",
    "        activations = self.activations.detach().cpu().numpy()[0]\n",
    "        gradients = gradients[1:]\n",
    "        activations = activations[1:]\n",
    "        weights = np.mean(gradients, axis=1)\n",
    "        cam = np.mean(activations, axis=1) * weights\n",
    "        \n",
    "        patch_size = 16\n",
    "        num_patches = int(np.sqrt(cam.shape[0]))\n",
    "        cam = cam.reshape(num_patches, num_patches)\n",
    "        cam = np.maximum(cam, 0)\n",
    "        if np.max(cam) > 0:\n",
    "            cam = cam / np.max(cam)\n",
    "        \n",
    "        cam = cv2.resize(cam, (input_image.shape[3], input_image.shape[2]))\n",
    "        return cam, target_class\n",
    "\n",
    "\n",
    "class MobileViTGradCAM:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.target_layer = model.stages[-1]\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        self.hooks = []\n",
    "        self.register_hooks()\n",
    "        \n",
    "    def register_hooks(self):\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output\n",
    "            \n",
    "        def backward_hook(module, grad_input, grad_output):\n",
    "            self.gradients = grad_output[0]\n",
    "        \n",
    "        self.hooks.append(self.target_layer.register_forward_hook(forward_hook))\n",
    "        self.hooks.append(self.target_layer.register_full_backward_hook(backward_hook))\n",
    "        \n",
    "    def remove_hooks(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "        \n",
    "    def generate_cam(self, input_image, target_class=None):\n",
    "        model_output = self.model(input_image)\n",
    "        \n",
    "        if target_class is None:\n",
    "            target_class = torch.argmax(model_output, dim=1).item()\n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        one_hot = torch.zeros_like(model_output)\n",
    "        one_hot[0, target_class] = 1\n",
    "        model_output.backward(gradient=one_hot, retain_graph=True)\n",
    "        \n",
    "        gradients = self.gradients.detach().cpu().numpy()[0]\n",
    "        activations = self.activations.detach().cpu().numpy()[0]\n",
    "        weights = np.mean(gradients, axis=(1, 2))\n",
    "        \n",
    "        cam = np.zeros(activations.shape[1:], dtype=np.float32)\n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * activations[i]\n",
    "        \n",
    "        cam = np.maximum(cam, 0)\n",
    "        if np.max(cam) > 0:\n",
    "            cam = cam / np.max(cam)\n",
    "        \n",
    "        cam = cv2.resize(cam, (input_image.shape[3], input_image.shape[2]))\n",
    "        return cam, target_class\n",
    "\n",
    "def load_model_bundle(bundle_path):\n",
    "    \"\"\"\n",
    "    Load a model bundle and return the model, transform, and model type\n",
    "    \"\"\"\n",
    "    bundle = torch.load(bundle_path, map_location=\"cpu\", weights_only=False)\n",
    "    model_name = bundle[\"model_name\"]\n",
    "    \n",
    "    if \"vit\" in model_name and \"mobilevit\" not in model_name:\n",
    "        model = timm.create_model(model_name, pretrained=False, num_classes=bundle[\"num_classes\"])\n",
    "        model_type = \"vit\"\n",
    "    elif \"resnet\" in model_name:\n",
    "        model = models.resnet50(pretrained=False)\n",
    "        model.fc = nn.Linear(model.fc.in_features, bundle[\"num_classes\"])\n",
    "        model_type = \"resnet\"\n",
    "    elif \"mobilevit\" in model_name:\n",
    "        model = timm.create_model(model_name, pretrained=False, num_classes=bundle[\"num_classes\"])\n",
    "        model_type = \"mobilevit\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_name}\")\n",
    "    \n",
    "    model.load_state_dict(bundle[\"state_dict\"])\n",
    "    model.eval()\n",
    "    \n",
    "    return model, bundle[\"transform\"], model_type\n",
    "\n",
    "def show_cam_on_image(img, mask, alpha=0.5):\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
    "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "    img_uint8 = np.uint8(255 * img)\n",
    "    cam_img = heatmap * alpha + img_uint8 * (1 - alpha)\n",
    "    cam_img = np.uint8(cam_img)\n",
    "    return cam_img\n",
    "\n",
    "def get_feature_map(model, image, layer_name, model_type):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    \n",
    "    def hook(module, input, output):\n",
    "        features.append(output.detach())\n",
    "    \n",
    "    if model_type == \"vit\":\n",
    "        handle = model.blocks[-1].register_forward_hook(hook)\n",
    "    elif model_type == \"resnet\":\n",
    "        handle = model.layer4.register_forward_hook(hook)\n",
    "    elif model_type == \"mobilevit\":\n",
    "        handle = model.stages[-1].register_forward_hook(hook)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model(image.unsqueeze(0))\n",
    "    handle.remove()\n",
    "    \n",
    "    feature_map = features[0][0].cpu().numpy()\n",
    "    if model_type == \"vit\":\n",
    "        if feature_map.shape[0] == 197:\n",
    "            feature_map = feature_map[1:]\n",
    "        feature_map = feature_map.mean(axis=1)\n",
    "        if feature_map.shape[0] == 196:\n",
    "            feature_map = feature_map.reshape(14, 14)\n",
    "    elif model_type in [\"resnet\", \"mobilevit\"]:\n",
    "        feature_map = feature_map.mean(axis=0)\n",
    "    \n",
    "    return feature_map\n",
    "\n",
    "def compute_accuracy(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = outputs.max(1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    return accuracy_score(all_labels, all_preds)\n",
    "\n",
    "def interactive_gradcam_viewer():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    \n",
    "    print(\"Loading ViT model...\")\n",
    "    vit_model, vit_transform, vit_type = load_model_bundle(vit_bundle_path)\n",
    "    vit_model = vit_model.to(device)\n",
    "    \n",
    "    print(\"Loading ResNet model...\")\n",
    "    resnet_model, resnet_transform, resnet_type = load_model_bundle(resnet_bundle_path)\n",
    "    resnet_model = resnet_model.to(device)\n",
    "    \n",
    "    print(\"Loading MobileViT model...\")\n",
    "    mobilevit_model, mobilevit_transform, mobilevit_type = load_model_bundle(mobilevit_bundle_path)\n",
    "    mobilevit_model = mobilevit_model.to(device)\n",
    "    \n",
    "    \n",
    "    vit_gradcam = ViTGradCAM(vit_model)\n",
    "    resnet_gradcam = ResNetGradCAM(resnet_model)\n",
    "    mobilevit_gradcam = MobileViTGradCAM(mobilevit_model)\n",
    "    \n",
    "    \n",
    "    vit_dataset = datasets.ImageFolder(data_dir, transform=vit_transform)\n",
    "    resnet_dataset = datasets.ImageFolder(data_dir, transform=resnet_transform)\n",
    "    mobilevit_dataset = datasets.ImageFolder(data_dir, transform=mobilevit_transform)\n",
    "    class_names = vit_dataset.classes\n",
    "    \n",
    "    \n",
    "    batch_size = 32\n",
    "    vit_loader = DataLoader(vit_dataset, batch_size=batch_size, shuffle=False)\n",
    "    resnet_loader = DataLoader(resnet_dataset, batch_size=batch_size, shuffle=False)\n",
    "    mobilevit_loader = DataLoader(mobilevit_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    \n",
    "    vit_acc = compute_accuracy(vit_model, vit_loader, device)\n",
    "    resnet_acc = compute_accuracy(resnet_model, resnet_loader, device)\n",
    "    mobilevit_acc = compute_accuracy(mobilevit_model, mobilevit_loader, device)\n",
    "    print(f\"ViT Test Accuracy: {vit_acc:.4f}\")\n",
    "    print(f\"ResNet Test Accuracy: {resnet_acc:.4f}\")\n",
    "    print(f\"MobileViT Test Accuracy: {mobilevit_acc:.4f}\")\n",
    "    \n",
    "    \n",
    "    img_loader = DataLoader(vit_dataset, batch_size=64, shuffle=True)\n",
    "    images, labels = next(iter(img_loader))\n",
    "    \n",
    "    \n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    \n",
    "    def update_view(idx=0):\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        img = images[idx].to(device)\n",
    "        label = labels[idx].item()\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            vit_output = vit_model(img.unsqueeze(0))\n",
    "            vit_pred = torch.argmax(vit_output, dim=1).item()\n",
    "            resnet_output = resnet_model(img.unsqueeze(0))\n",
    "            resnet_pred = torch.argmax(resnet_output, dim=1).item()\n",
    "            mobilevit_output = mobilevit_model(img.unsqueeze(0))\n",
    "            mobilevit_pred = torch.argmax(mobilevit_output, dim=1).item()\n",
    "        \n",
    "        \n",
    "        vit_cam, _ = vit_gradcam.generate_cam(img.unsqueeze(0), target_class=None)\n",
    "        resnet_cam, _ = resnet_gradcam.generate_cam(img.unsqueeze(0), target_class=None)\n",
    "        mobilevit_cam, _ = mobilevit_gradcam.generate_cam(img.unsqueeze(0), target_class=None)\n",
    "        \n",
    "        \n",
    "        vit_feature = get_feature_map(vit_model, img, \"blocks\", model_type=\"vit\")\n",
    "        resnet_feature = get_feature_map(resnet_model, img, \"layer4\", model_type=\"resnet\")\n",
    "        mobilevit_feature = get_feature_map(mobilevit_model, img, \"stages\", model_type=\"mobilevit\")\n",
    "        \n",
    "        \n",
    "        img_np = img.cpu().numpy().transpose(1, 2, 0) * std + mean\n",
    "        img_np = np.clip(img_np, 0, 1)\n",
    "        \n",
    "        \n",
    "        vit_overlay = show_cam_on_image(img_np, vit_cam)\n",
    "        resnet_overlay = show_cam_on_image(img_np, resnet_cam)\n",
    "        mobilevit_overlay = show_cam_on_image(img_np, mobilevit_cam)\n",
    "        \n",
    "        \n",
    "        fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "        \n",
    "        \n",
    "        axes[0, 0].imshow(img_np)\n",
    "        axes[0, 0].set_title(f\"Original - Class: {class_names[label]}\")\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        \n",
    "        axes[0, 1].imshow(vit_overlay)\n",
    "        axes[0, 1].set_title(f\"ViT GradCAM - Pred: {class_names[vit_pred]}\")\n",
    "        axes[0, 1].axis('off')\n",
    "        \n",
    "        \n",
    "        axes[0, 2].imshow(resnet_overlay)\n",
    "        axes[0, 2].set_title(f\"ResNet GradCAM - Pred: {class_names[resnet_pred]}\")\n",
    "        axes[0, 2].axis('off')\n",
    "        \n",
    "        \n",
    "        axes[0, 3].imshow(mobilevit_overlay)\n",
    "        axes[0, 3].set_title(f\"MobileViT GradCAM - Pred: {class_names[mobilevit_pred]}\")\n",
    "        axes[0, 3].axis('off')\n",
    "        \n",
    "        \n",
    "        axes[1, 0].imshow(img_np)\n",
    "        axes[1, 0].set_title(\"Original Image\")\n",
    "        axes[1, 0].axis('off')\n",
    "        \n",
    "        axes[1, 1].imshow(vit_feature, cmap='viridis')\n",
    "        axes[1, 1].set_title(\"ViT Feature Map\")\n",
    "        axes[1, 1].axis('off')\n",
    "        \n",
    "        axes[1, 2].imshow(resnet_feature, cmap='viridis')\n",
    "        axes[1, 2].set_title(\"ResNet Feature Map\")\n",
    "        axes[1, 2].axis('off')\n",
    "        \n",
    "        axes[1, 3].imshow(mobilevit_feature, cmap='viridis')\n",
    "        axes[1, 3].set_title(\"MobileViT Feature Map\")\n",
    "        axes[1, 3].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        print(f\"True Class: {class_names[label]}\")\n",
    "        print(f\"ViT Prediction: {class_names[vit_pred]} {'' if vit_pred == label else ''}\")\n",
    "        print(f\"ResNet Prediction: {class_names[resnet_pred]} {'' if resnet_pred == label else ''}\")\n",
    "        print(f\"MobileViT Prediction: {class_names[mobilevit_pred]} {'' if mobilevit_pred == label else ''}\")\n",
    "    \n",
    "    image_slider = widgets.IntSlider(min=0, max=len(images)-1, step=1, value=0, description='Image:')\n",
    "    widgets.interact(update_view, idx=image_slider)\n",
    "    \n",
    "    vit_gradcam.remove_hooks()\n",
    "    resnet_gradcam.remove_hooks()\n",
    "    mobilevit_gradcam.remove_hooks()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    interactive_gradcam_viewer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
